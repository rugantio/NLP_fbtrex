{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argentine Election Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook I analyze a Spanish dataset set up during the [Argentine legislative election](https://en.wikipedia.org/wiki/Argentine_legislative_election,_2017) of 2017. \n",
    "This dataset contains the data of 9 facebook bots, crawled over a period of 16 days, following 45 sources.\n",
    "\n",
    "__Note__: If you haven't done it already, go through the set up in the *README* of [this repo](https://github.com/rugantio/nlp_fbtrex/).\n",
    "\n",
    "### Roadmap\n",
    "Download dataset -> cast JSON to txt -> tokenization -> normalization -> phrase modeling -> topic mining -> burst the bubble -> word2vec algebra & predictive analysis\n",
    "\n",
    "## Dataset\n",
    "The dataset was prepared by the [__Facebook Tracking Exposed__](https://facebook.tracking.exposed/) project and can be retrieved in a convenient JSON format from the specific GitHub [__repo__](https://github.com/tracking-exposed/experiments-data/tree/master/silver).\n",
    "There are two separate files that we'll try to breakdown:\n",
    "* __fbtrex-data-\\*.json__ - Contains all impressions relative to single users\n",
    "* __semantic-entities.json__ - Contains all available metadata regarding posts\n",
    "\n",
    "The text field of every posts is enclosed in *semantic-entities.json*, while I can use *fbtrex-data-\\*.json* to correlate which user has visualized this content, thus providing an easy way to investigate the Facebook filter bubble.\n",
    "Given a ready working environment, as explained is the *README* of this repo, just go ahead and download the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Download Argentine dataset in a data subdir\n",
    "mkdir data && cd data\n",
    "wget https://github.com/tracking-exposed/experiments-data/raw/master/silver/fbtrex-data-1.json.zip\n",
    "wget https://github.com/tracking-exposed/experiments-data/raw/master/silver/semantic-entities.json.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: This commands are supposed to be executed in a bash environment, not in the notebook itself. The operation may fail due to permissions.\n",
    "\n",
    "Extract the content from the zip archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Extract JSON from zipped archives\n",
    "cd data\n",
    "unzip fbtrex-data-1.json.zip\n",
    "unzip semantic-entities.json.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: To try out this notebook I made a shorter version of the JSON, I highly recommend to do the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the dataset in JSON format, we can use the [JSON Python library](https://docs.python.org/3/library/json.html) to decode its content and store it in a Python variable. The variable type depends on the actual content of the provided file, by [default](https://docs.python.org/3/library/json.html#json-to-py-table) a JSON object is decoded to a dict and an arrays to a list. The recommended approach for working with encoded text files, is to use the [codecs Python library](https://docs.python.org/3/library/codecs.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "with codecs.open('data/semantic-entities.json',encoding='utf-8') as data_json:    \n",
    "    data = json.load(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print to stdout the content of the parsed JSON file just use [pprint](https://docs.python.org/3/library/pprint.html), the data pretty printer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to check if the casting was performed correctly before proceding, the resulting decoded type can be inspected with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you are using Spyder IDE you can keep track of variable simply looking at the variable explorer window.\n",
    "\n",
    "So the JSON is now a list. How many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 437 total elements to analyze.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} total elements to analyze.'.format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go deeper. We decoded the JSON to a list, but what kind of list is it? What happened to JSON objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, *data* is not a simple list, it's a nested list of dictionaries! Let's print the *dict_keys*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    print(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: in the provided dataset there are some entities that don't have a *text* field, due to some errors of the parser. So let's first take only the elements that have a text field and put them in a new non-nested list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tex = [item['text'] for item in data if 'text' in item]\n",
    "print (tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. We now have an actual working list. Again, how many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are actually 429 text elements to analyze!\n"
     ]
    }
   ],
   "source": [
    "print('There are actually {} text elements to analyze!'.format(len(tex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good enough for now, later we can make a deeper analysis, associating each *text* key with its *id* key and its *time* key to correlate which user visualizes which entity and when.  \n",
    "\n",
    "It's good practice to have a new txt file for every step in NLP processing. So let's create a new txt file populated with the *text keys* of the *tex list*, __one per line__. \n",
    "\n",
    "Since some of the text values are made of more than one paragraphs, we need to substitute linebreaks (newline character) with a space character. Some caution is needed because some paragraphs have a double linebreak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Swap linebreaks with a space\n",
    "for i in range(len(tex)):\n",
    "    tex[i] = tex[i].replace('\\n\\n','\\n')\n",
    "    tex[i] = tex[i].replace('\\n',' ')\n",
    "\n",
    "#Create new txt with text keys (one per line)\n",
    "with codecs.open('data/text.txt','w',encoding='utf-8') as text:\n",
    "    for i in range(len(tex)):\n",
    "        text.write('%s\\n' % tex[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the file and check that everything was executed as it should you don't need another editor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerró el comedor cordobés al que Macri le había prometido ayuda Luis Almadaes cordobés ytenía un comedor comunitario y una fundación para ayudar a gente en situación de calle. Desesperado, en mayo le envió un mensaje por redes sociales al presidente Mauricio Macri para que lo ayude a sostener el emprendimiento. El mandatario lo llamó por teléfono días después y el 12 de julio se vieron personalmente en esa provincia. Allí, le prometió ayuda. Pero nunca llegó y por eso no tuvo más alternativa que bajar la persiana. Su fundación \"Yo Te Ayudo Amigo del Alma\" ayudaba a unas 20 personas en situación de calle para que vendieran golosinas en la peatonal y pudieran capacitarse en un oficio. Almada dijo a La Naciónque debió cerrar la fundación: \"Puse mi esfuerzo,cumplí con lo que me comprometí pero solo no puedo. No llamé al Presidente para pedirle alfajores, sino para que nos ayudara con tratamiento de adicciones, con asistentes sociales. Macri me dijo que lo hagamos.Los tiempos de Nación no son los mismos que los de la gente\". \"Me fueron pateando primero dos meses, después tres y la semana pasada me dijeron cuatro meses -continuó-. Puse plata de mi bolsillo, pero no puedo más. No pedí nada para mí, era para la gente. Vino el Presidente; estamos haciendo un laburo donde tendrían que estar ellos\".  Almada dijo que son \"requisitos, y más requisitos, y nos piden un salón que sale $20.000 aparte del que estamos pagando; la Provincia nos dio alojamiento para los muchachos y la Municipalidad nos dio el permiso para la venta callejera. El proyecto quedó rengo\", contó. Hace unos días habló telefónicamente con la ministra Carolina Stanley y, según señaló, envió a una asesora.  No me reuní con un concejal, sino con el Presidente y para mí su palabra vale \"Ahora me dicen 'Almada no le dijimos que empezara'. ¿Cómo me van a contar eso? . No tengo más plata, lo positivo es que de los 30 muchachos con los que empecé a cuatro le conseguimos trabajo. Laburo diez horas por día para la sociedad, pero no puedo más; ahora la pelota está en la cancha de ellos\". El día de la visita presidencial, el cordobés contó: “Con el presidente carancheamos unos pollos de la fuente”. Luego, le regaló una camiseta de Talleres con el nombre del presidente. Sin embargo, luego de anunciar el cese de actividades de la fundación, Almada emitió un comunicado a través de las redes sociales en el que afirma que seguirá trabajando \"con el Estado Nacional\" para ayudar a las personas \"en situación de calle\". \"Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar. Ello nos obliga a seguirlos ayudando y reevaluar la estrategia\", explica el comunicado. “Yo te ayudo Amigo del Alma” ratifica que va a seguir trabajando con el Estado Nacional para ayudar a personas en situación de calle en Córdoba. Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar. Ello nos obliga a seguirlos ayudando y reevaluar la estrategia. Cuando hablamos con el gobierno se comprometieron a ayudarnos. El Ministerio de Desarrollo Social de la Nación nos habían pedido algunos requisitos para que todo sea de manera transparente. Mientras no los tuvimos nosotros salimos por nuestra cuenta y a cuanta gente pudimos Le dimos Comida, Le dimos Trabajo y Le dimos Albergue, pero se nos hizo imposible luchar contra el flagelo de la droga. Este flagelo esta contaminando gravemente nuestra población, principalmente a los que menos tienen. Entendimos que no alcanza con alimentarlos, darles trabajo y la posibilidad de dormir en un albergue. Ayer tuvimos que parar la pelota, no podemos resolver este problema desde la fundación. Le generamos un empleo, dinero en su bolsillo y los terminamos dejando solos en la pelea contra la droga. Acordamos que estos meses sirvieron para fortalecer el conocimiento compartido sobre como abordar este flagelo. El gobierno nos acompaña en esta temática para que los recursos económicos y de conocimiento que nos aporte sean bien utilizados y ayuden de verdad a la gente a salir de este problema. Tanto nosotros cómo el Estado entendemos que todo esto significa un importante proceso de aprendizaje que nos servirá para continuar haciendo lo que queremos hacer. Entendemos, además, que el Estado debe cuidar el dinero de los argentinos. Nosotros vimos una situación de vulnerabilidad y quisimos ayudar. Desde el Estado se comprometieron a seguir redoblando esfuerzos para lograr concretar este sueño sin que sea perjudicial para nadie. Por eso es necesario que todos sepamos que detrás de todo esto hay historias de vida, que no es fácil y que hay que tener paciencia y trabajo en equipo para ayudar a los que más lo necesitan. Esta gente en situación de calle, ES GENTE BUENA Y TODOS TENEMOS QUE AYUDARLOS. ESTAN SOLOS Y DESPROTEGIDOS. Ratificamos nuestras ganas, buscaremos la manera y cuando estén dadas todas las condiciones necesarias, en conjunto con el gobierno nacional, retomaremos el trabajo juntos para sacar a todas las personas que podamos de la calle, darles formación y trabajo para que puedan vivir dignamente.\n",
      "Detuvieron a un profesor universitario acusado de tirarle huevos a Macri El profesor universitario Aníbal Prina, acusado de haber tirado huevos contra la comitiva presidencial, en el marco de la visita que hizo el jueves el jefe de Estado Mauricio Macri a Santa Rosa, fue detenido este viernes por la Policía Federal. Prina fue el tercer candidato a diputado nacional por el kirchnerista Frente Peronista Barrial y quedó arrestado en la Facultad de Agronomía. El docente está acusado de cometer el delito de intimidación publica estipulado por el artículo 211 del Código Penal. En principio, Prina estuvo demorado durante dos horas en la Facultad de Agronomía, ubicada en la ru\n"
     ]
    }
   ],
   "source": [
    "#Print the first 3000 characters\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:    \n",
    "    print(text.read(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is over, we now have a txt ready to feed our NLP modules!\n",
    "## Language processing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining tasks have become incredibly easy thanks to [spaCy](http://alpha.spacy.io/), a NLP Python module which provides:\n",
    "* Non-destructive tokenization\n",
    "* Syntax-driven sentence segmentation\n",
    "* Pre-trained word vectors\n",
    "* Part-of-speech tagging\n",
    "* Named entity recognition\n",
    "* Labelled dependency parsing\n",
    "* A built-in visualizer \n",
    "\n",
    "...and much more, all with just one function!\n",
    "\n",
    "SpaCy also provides some already trained [models](https://alpha.spacy.io/models/) which you can use out-of-the-box to process different languages. SpaCy's core is written in pure C (via Cython), it's currently the [fastest](https://alpha.spacy.io/usage/facts-figures) parser available and makes [multithreading](https://explosion.ai/blog/multithreading-with-cython) profitable by virtue of Cython.\n",
    "\n",
    "Follow the *README* of this repo and install the Spanish language model. Now import the model, and load spaCy's pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 303 ms, sys: 34.1 ms, total: 337 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import spacy\n",
    "\n",
    "#Initialize SpaCy's pipeline\n",
    "nlp = spacy.load('es_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a processing pipeline, we can call a *nlp* instance as if it were a function on a string of text. This will produce a [Doc](https://alpha.spacy.io/api/doc) object, a special container that holds all linguistic annotations of the text fed in.\n",
    "\n",
    "Let's first explore how SpaCy processes a single entity, before diving into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 502 ms, sys: 1.06 s, total: 1.56 s\n",
      "Wall time: 696 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Snip single line of text\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:\n",
    "    line_txt = text.readline()\n",
    "\n",
    "#Standard way of processing text \n",
    "doc = nlp(line_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerró el comedor cordobés al que Macri le había prometido ayuda Luis Almadaes cordobés ytenía un comedor comunitario y una fundación para ayudar a gente en situación de calle. Desesperado, en mayo le envió un mensaje por redes sociales al presidente Mauricio Macri para que lo ayude a sostener el emprendimiento. El mandatario lo llamó por teléfono días después y el 12 de julio se vieron personalmente en esa provincia. Allí, le prometió ayuda. Pero nunca llegó y por eso no tuvo más alternativa que bajar la persiana. Su fundación \"Yo Te Ayudo Amigo del Alma\" ayudaba a unas 20 personas en situación de calle para que vendieran golosinas en la peatonal y pudieran capacitarse en un oficio. Almada dijo a La Naciónque debió cerrar la fundación: \"Puse mi esfuerzo,cumplí con lo que me comprometí pero solo no puedo. No llamé al Presidente para pedirle alfajores, sino para que nos ayudara con tratamiento de adicciones, con asistentes sociales. Macri me dijo que lo hagamos.Los tiempos de Nación no son los mismos que los de la gente\". \"Me fueron pateando primero dos meses, después tres y la semana pasada me dijeron cuatro meses -continuó-. Puse plata de mi bolsillo, pero no puedo más. No pedí nada para mí, era para la gente. Vino el Presidente; estamos haciendo un laburo donde tendrían que estar ellos\".  Almada dijo que son \"requisitos, y más requisitos, y nos piden un salón que sale $20.000 aparte del que estamos pagando; la Provincia nos dio alojamiento para los muchachos y la Municipalidad nos dio el permiso para la venta callejera. El proyecto quedó rengo\", contó. Hace unos días habló telefónicamente con la ministra Carolina Stanley y, según señaló, envió a una asesora.  No me reuní con un concejal, sino con el Presidente y para mí su palabra vale \"Ahora me dicen 'Almada no le dijimos que empezara'. ¿Cómo me van a contar eso? . No tengo más plata, lo positivo es que de los 30 muchachos con los que empecé a cuatro le conseguimos trabajo. Laburo diez horas por día para la sociedad, pero no puedo más; ahora la pelota está en la cancha de ellos\". El día de la visita presidencial, el cordobés contó: “Con el presidente carancheamos unos pollos de la fuente”. Luego, le regaló una camiseta de Talleres con el nombre del presidente. Sin embargo, luego de anunciar el cese de actividades de la fundación, Almada emitió un comunicado a través de las redes sociales en el que afirma que seguirá trabajando \"con el Estado Nacional\" para ayudar a las personas \"en situación de calle\". \"Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar. Ello nos obliga a seguirlos ayudando y reevaluar la estrategia\", explica el comunicado. “Yo te ayudo Amigo del Alma” ratifica que va a seguir trabajando con el Estado Nacional para ayudar a personas en situación de calle en Córdoba. Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar. Ello nos obliga a seguirlos ayudando y reevaluar la estrategia. Cuando hablamos con el gobierno se comprometieron a ayudarnos. El Ministerio de Desarrollo Social de la Nación nos habían pedido algunos requisitos para que todo sea de manera transparente. Mientras no los tuvimos nosotros salimos por nuestra cuenta y a cuanta gente pudimos Le dimos Comida, Le dimos Trabajo y Le dimos Albergue, pero se nos hizo imposible luchar contra el flagelo de la droga. Este flagelo esta contaminando gravemente nuestra población, principalmente a los que menos tienen. Entendimos que no alcanza con alimentarlos, darles trabajo y la posibilidad de dormir en un albergue. Ayer tuvimos que parar la pelota, no podemos resolver este problema desde la fundación. Le generamos un empleo, dinero en su bolsillo y los terminamos dejando solos en la pelea contra la droga. Acordamos que estos meses sirvieron para fortalecer el conocimiento compartido sobre como abordar este flagelo. El gobierno nos acompaña en esta temática para que los recursos económicos y de conocimiento que nos aporte sean bien utilizados y ayuden de verdad a la gente a salir de este problema. Tanto nosotros cómo el Estado entendemos que todo esto significa un importante proceso de aprendizaje que nos servirá para continuar haciendo lo que queremos hacer. Entendemos, además, que el Estado debe cuidar el dinero de los argentinos. Nosotros vimos una situación de vulnerabilidad y quisimos ayudar. Desde el Estado se comprometieron a seguir redoblando esfuerzos para lograr concretar este sueño sin que sea perjudicial para nadie. Por eso es necesario que todos sepamos que detrás de todo esto hay historias de vida, que no es fácil y que hay que tener paciencia y trabajo en equipo para ayudar a los que más lo necesitan. Esta gente en situación de calle, ES GENTE BUENA Y TODOS TENEMOS QUE AYUDARLOS. ESTAN SOLOS Y DESPROTEGIDOS. Ratificamos nuestras ganas, buscaremos la manera y cuando estén dadas todas las condiciones necesarias, en conjunto con el gobierno nacional, retomaremos el trabajo juntos para sacar a todas las personas que podamos de la calle, darles formación y trabajo para que puedan vivir dignamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks exactly the same! But what happened under the hood? Have a look at how [spaCy's pipeline](https://alpha.spacy.io/usage/processing-pipelines) is made:\n",
    "\n",
    "__Text -> tokenizer -> tagger -> parser -> ner -> Doc__\n",
    "\n",
    "Text analysis is built from bottom-up. The *tokenizer* creates a *Doc* data structure, breaking the text in tokens and storing their metadata in a tensor. The *tagger* takes these tokens (and their context) and uses the information to make predictions of the part-of-speech tags. The *parser* assigns dependency labels between tokens and segments text in sentences. The *ner*, named entity recognizer, detects and labels named entities.\n",
    "### Sentence detection and segmentation\n",
    "Sentences are automatically extracted from each post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Cerró el comedor cordobés al que Macri le había prometido ayuda Luis Almadaes cordobés ytenía un comedor comunitario y una fundación para ayudar a gente en situación de calle.\n",
      "Sentence 2: Desesperado, en mayo le envió un mensaje por redes sociales al presidente Mauricio Macri para que lo ayude a sostener el emprendimiento. \n",
      "Sentence 3: El mandatario lo llamó por teléfono días después y el 12 de julio se vieron personalmente en esa provincia.\n",
      "Sentence 4: Allí, le prometió ayuda. \n",
      "Sentence 5: Pero nunca llegó y por eso no tuvo más alternativa que bajar la persiana.\n",
      "Sentence 6: Su fundación \"Yo Te Ayudo Amigo del Alma\" ayudaba a unas 20 personas en situación de calle para que vendieran golosinas en la peatonal y pudieran capacitarse en un oficio.\n",
      "Sentence 7: Almada dijo a La Naciónque debió cerrar la fundación: \"Puse mi esfuerzo,cumplí con lo que me comprometí pero solo no puedo.\n",
      "Sentence 8: No llamé al Presidente para pedirle alfajores, sino para que nos ayudara con tratamiento de adicciones, con asistentes sociales.\n",
      "Sentence 9: Macri me dijo que lo hagamos.\n",
      "Sentence 10: Los tiempos de Nación no son los mismos que los de la gente\".\n",
      "Sentence 11: \"Me fueron pateando primero dos meses, después tres y la semana pasada me dijeron cuatro meses -continuó-.\n",
      "Sentence 12: Puse plata de mi bolsillo, pero no puedo más.\n",
      "Sentence 13: No pedí nada para mí, era para la gente.\n",
      "Sentence 14: Vino el Presidente; estamos haciendo un laburo donde tendrían que estar ellos\".  \n",
      "Sentence 15: Almada dijo que son \"requisitos, y más requisitos, y nos piden un salón que sale $20.000 aparte del que estamos pagando; la Provincia nos dio alojamiento para los muchachos y la Municipalidad nos dio el permiso para la venta callejera.\n",
      "Sentence 16: El proyecto quedó rengo\", contó.\n",
      "Sentence 17: Hace unos días habló telefónicamente con la ministra Carolina Stanley y, según señaló, envió a una asesora.  \n",
      "Sentence 18: No me reuní con un concejal, sino con el Presidente y para mí su palabra vale \"Ahora me dicen 'Almada no le dijimos que empezara'.\n",
      "Sentence 19: ¿Cómo me van a contar eso? .\n",
      "Sentence 20: No tengo más plata, lo positivo es que de los 30 muchachos con los que empecé a cuatro le conseguimos trabajo.\n",
      "Sentence 21: Laburo diez horas por día para la sociedad, pero no puedo más; ahora la pelota está en la cancha de ellos\".\n",
      "Sentence 22: El día de la visita presidencial, el cordobés contó: “Con el presidente carancheamos unos pollos de la fuente”.\n",
      "Sentence 23: Luego, le regaló una camiseta de Talleres con el nombre del presidente.\n",
      "Sentence 24: Sin embargo, luego de anunciar el cese de actividades de la fundación, Almada emitió un comunicado a través de las redes sociales en el que afirma que seguirá trabajando \"con el Estado Nacional\" para ayudar a las personas \"en situación de calle\".\n",
      "Sentence 25: \"Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar.\n",
      "Sentence 26: Ello nos obliga a seguirlos ayudando y reevaluar la estrategia\", explica el comunicado.\n",
      "Sentence 27: “Yo te ayudo Amigo del Alma” ratifica que va a seguir trabajando con el Estado Nacional para ayudar a personas en situación de calle en Córdoba.\n",
      "Sentence 28: Ayer recibimos el pedido desesperado de la gente a la que ayudábamos, también se comunicaron con nosotros personas con ganas de involucrarse y ayudar.\n",
      "Sentence 29: Ello nos obliga a seguirlos ayudando y reevaluar la estrategia.\n",
      "Sentence 30: Cuando hablamos con el gobierno se comprometieron a ayudarnos.\n",
      "Sentence 31: El Ministerio de Desarrollo Social de la Nación nos habían pedido algunos requisitos para que todo sea de manera transparente.\n",
      "Sentence 32: Mientras no los tuvimos nosotros salimos por nuestra cuenta y a cuanta gente pudimos Le dimos Comida, Le dimos Trabajo y Le dimos Albergue, pero se nos hizo imposible luchar contra el flagelo de la droga.\n",
      "Sentence 33: Este flagelo esta contaminando gravemente nuestra población, principalmente a los que menos tienen.\n",
      "Sentence 34: Entendimos que no alcanza con alimentarlos, darles trabajo y la posibilidad de dormir en un albergue.\n",
      "Sentence 35: Ayer tuvimos que parar la pelota, no podemos resolver este problema desde la fundación.\n",
      "Sentence 36: Le generamos un empleo, dinero en su bolsillo y los terminamos dejando solos en la pelea contra la droga.\n",
      "Sentence 37: Acordamos que estos meses sirvieron para fortalecer el conocimiento compartido sobre como abordar este flagelo.\n",
      "Sentence 38: El gobierno nos acompaña en esta temática para que los recursos económicos y de conocimiento que nos aporte sean bien utilizados y ayuden de verdad a la gente a salir de este problema.\n",
      "Sentence 39: Tanto nosotros cómo el Estado entendemos que todo esto significa un importante proceso de aprendizaje que nos servirá para continuar haciendo lo que queremos hacer.\n",
      "Sentence 40: Entendemos, además, que el Estado debe cuidar el dinero de los argentinos.\n",
      "Sentence 41: Nosotros vimos una situación de vulnerabilidad y quisimos ayudar.\n",
      "Sentence 42: Desde el Estado se comprometieron a seguir redoblando esfuerzos para lograr concretar este sueño sin que sea perjudicial para nadie.\n",
      "Sentence 43: Por eso es necesario que todos sepamos que detrás de todo esto hay historias de vida, que no es fácil y que hay que tener paciencia y trabajo en equipo para ayudar a los que más lo necesitan.\n",
      "Sentence 44: Esta gente en situación de calle, ES GENTE BUENA Y TODOS TENEMOS QUE AYUDARLOS.\n",
      "Sentence 45: ESTAN SOLOS Y DESPROTEGIDOS.\n",
      "Sentence 46: Ratificamos nuestras ganas, buscaremos la manera y cuando estén dadas todas las condiciones necesarias, en conjunto con el gobierno nacional, retomaremos el trabajo juntos para sacar a todas las personas que podamos de la calle, darles formación y trabajo para que puedan vivir dignamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print ('Sentence {}:'.format(i + 1),sent,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging and grammar analysis\n",
    "Using [Pandas](http://pandas.pydata.org/), Python Data Analysis library, we can have a clean table visualization.\n",
    "- Text: The original word text.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag, with full morphology!\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerró</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__Mood=Ind|Number=Sing|Person=3|Tense=Past...</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Def|Gender=Masc|Number=Sing|Pron...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Masc|Number=Sing</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Number=Sing</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Preppron|Gender=Masc|Number=Sing</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>que</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON__PronType=Rel</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Macri</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN___</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON__Case=Dat|Number=Sing|Person=3|PronType=Prs</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>había</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX__Mood=Ind|Number=Sing|Person=3|Tense=Imp|V...</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__Gender=Masc|Number=Sing|Tense=Past|VerbF...</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayuda</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Luis</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN___</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Almadaes</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN___</td>\n",
       "      <td>flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Number=Plur</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ytenía</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__Mood=Ind|Number=Sing|Person=3|Tense=Imp|...</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Ind|Gender=Masc|Number=Sing|Pron...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Masc|Number=Sing</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comunitario</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Gender=Masc|Number=Sing</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CCONJ___</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>una</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Ind|Gender=Fem|Number=Sing|PronT...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fundación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>para</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ayudar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__VerbForm=Inf</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gente</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>situación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>obl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>de</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>calle</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT__PunctType=Peri</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>gobierno</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Masc|Number=Sing</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>nacional</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Number=Sing</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT__PunctType=Comm</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>retomaremos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__Mood=Ind|Number=Plur|Person=1|Tense=Pres...</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>el</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Def|Gender=Masc|Number=Sing|Pron...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Masc|Number=Sing</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>juntos</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Gender=Masc|Number=Plur</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>para</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>sacar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__VerbForm=Inf</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>a</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>todas</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Gender=Fem|Number=Plur|PronType=Ind</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>las</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Def|Gender=Fem|Number=Plur|PronT...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>personas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Plur</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>que</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON__PronType=Rel</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>podamos</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX__Mood=Sub|Number=Plur|Person=1|Tense=Pres|...</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>de</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>la</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET__Definite=Def|Gender=Fem|Number=Sing|PronT...</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>calle</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT__PunctType=Comm</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>darles</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ__Number=Plur</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>formación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Fem|Number=Sing</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>y</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CCONJ___</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN__Gender=Masc|Number=Sing</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>para</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP__AdpType=Prep</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>que</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>SCONJ___</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>puedan</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX__Mood=Sub|Number=Plur|Person=3|Tense=Pres|...</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>vivir</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB__VerbForm=Inf</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dignamente</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV___</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT__PunctType=Peri</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Text    POS                                                Tag  \\\n",
       "0          Cerró   VERB  VERB__Mood=Ind|Number=Sing|Person=3|Tense=Past...   \n",
       "1             el    DET  DET__Definite=Def|Gender=Masc|Number=Sing|Pron...   \n",
       "2        comedor   NOUN                      NOUN__Gender=Masc|Number=Sing   \n",
       "3       cordobés    ADJ                                   ADJ__Number=Sing   \n",
       "4             al    ADP      ADP__AdpType=Preppron|Gender=Masc|Number=Sing   \n",
       "5            que   PRON                                 PRON__PronType=Rel   \n",
       "6          Macri  PROPN                                           PROPN___   \n",
       "7             le   PRON   PRON__Case=Dat|Number=Sing|Person=3|PronType=Prs   \n",
       "8          había    AUX  AUX__Mood=Ind|Number=Sing|Person=3|Tense=Imp|V...   \n",
       "9      prometido   VERB  VERB__Gender=Masc|Number=Sing|Tense=Past|VerbF...   \n",
       "10         ayuda   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "11          Luis  PROPN                                           PROPN___   \n",
       "12      Almadaes  PROPN                                           PROPN___   \n",
       "13      cordobés    ADJ                                   ADJ__Number=Plur   \n",
       "14        ytenía   VERB  VERB__Mood=Ind|Number=Sing|Person=3|Tense=Imp|...   \n",
       "15            un    DET  DET__Definite=Ind|Gender=Masc|Number=Sing|Pron...   \n",
       "16       comedor   NOUN                      NOUN__Gender=Masc|Number=Sing   \n",
       "17   comunitario    ADJ                       ADJ__Gender=Masc|Number=Sing   \n",
       "18             y   CONJ                                           CCONJ___   \n",
       "19           una    DET  DET__Definite=Ind|Gender=Fem|Number=Sing|PronT...   \n",
       "20     fundación   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "21          para    ADP                                  ADP__AdpType=Prep   \n",
       "22        ayudar   VERB                                 VERB__VerbForm=Inf   \n",
       "23             a    ADP                                  ADP__AdpType=Prep   \n",
       "24         gente   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "25            en    ADP                                  ADP__AdpType=Prep   \n",
       "26     situación   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "27            de    ADP                                  ADP__AdpType=Prep   \n",
       "28         calle   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "29             .  PUNCT                              PUNCT__PunctType=Peri   \n",
       "..           ...    ...                                                ...   \n",
       "958     gobierno   NOUN                      NOUN__Gender=Masc|Number=Sing   \n",
       "959     nacional    ADJ                                   ADJ__Number=Sing   \n",
       "960            ,  PUNCT                              PUNCT__PunctType=Comm   \n",
       "961  retomaremos   VERB  VERB__Mood=Ind|Number=Plur|Person=1|Tense=Pres...   \n",
       "962           el    DET  DET__Definite=Def|Gender=Masc|Number=Sing|Pron...   \n",
       "963      trabajo   NOUN                      NOUN__Gender=Masc|Number=Sing   \n",
       "964       juntos    ADJ                       ADJ__Gender=Masc|Number=Plur   \n",
       "965         para    ADP                                  ADP__AdpType=Prep   \n",
       "966        sacar   VERB                                 VERB__VerbForm=Inf   \n",
       "967            a    ADP                                  ADP__AdpType=Prep   \n",
       "968        todas    DET           DET__Gender=Fem|Number=Plur|PronType=Ind   \n",
       "969          las    DET  DET__Definite=Def|Gender=Fem|Number=Plur|PronT...   \n",
       "970     personas   NOUN                       NOUN__Gender=Fem|Number=Plur   \n",
       "971          que   PRON                                 PRON__PronType=Rel   \n",
       "972      podamos    AUX  AUX__Mood=Sub|Number=Plur|Person=1|Tense=Pres|...   \n",
       "973           de    ADP                                  ADP__AdpType=Prep   \n",
       "974           la    DET  DET__Definite=Def|Gender=Fem|Number=Sing|PronT...   \n",
       "975        calle   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "976            ,  PUNCT                              PUNCT__PunctType=Comm   \n",
       "977       darles    ADJ                                   ADJ__Number=Plur   \n",
       "978    formación   NOUN                       NOUN__Gender=Fem|Number=Sing   \n",
       "979            y   CONJ                                           CCONJ___   \n",
       "980      trabajo   NOUN                      NOUN__Gender=Masc|Number=Sing   \n",
       "981         para    ADP                                  ADP__AdpType=Prep   \n",
       "982          que  SCONJ                                           SCONJ___   \n",
       "983       puedan    AUX  AUX__Mood=Sub|Number=Plur|Person=3|Tense=Pres|...   \n",
       "984        vivir   VERB                                 VERB__VerbForm=Inf   \n",
       "985   dignamente    ADV                                             ADV___   \n",
       "986            .  PUNCT                              PUNCT__PunctType=Peri   \n",
       "987           \\n  SPACE                                                      \n",
       "\n",
       "        Dep  \n",
       "0      ROOT  \n",
       "1       det  \n",
       "2     nsubj  \n",
       "3      amod  \n",
       "4      case  \n",
       "5       obj  \n",
       "6     nsubj  \n",
       "7       obj  \n",
       "8       aux  \n",
       "9       acl  \n",
       "10      obj  \n",
       "11    nsubj  \n",
       "12     flat  \n",
       "13    nsubj  \n",
       "14    advcl  \n",
       "15      det  \n",
       "16      obj  \n",
       "17     amod  \n",
       "18       cc  \n",
       "19      det  \n",
       "20     conj  \n",
       "21     mark  \n",
       "22      acl  \n",
       "23     case  \n",
       "24      obj  \n",
       "25     case  \n",
       "26      obl  \n",
       "27     case  \n",
       "28     nmod  \n",
       "29    punct  \n",
       "..      ...  \n",
       "958    nmod  \n",
       "959    amod  \n",
       "960   punct  \n",
       "961   advcl  \n",
       "962     det  \n",
       "963     obj  \n",
       "964    amod  \n",
       "965    mark  \n",
       "966   advcl  \n",
       "967    case  \n",
       "968     det  \n",
       "969     det  \n",
       "970     obj  \n",
       "971     obj  \n",
       "972     acl  \n",
       "973    case  \n",
       "974     det  \n",
       "975     obj  \n",
       "976   punct  \n",
       "977    amod  \n",
       "978   appos  \n",
       "979      cc  \n",
       "980    conj  \n",
       "981    mark  \n",
       "982    mark  \n",
       "983     aux  \n",
       "984   advcl  \n",
       "985  advmod  \n",
       "986   punct  \n",
       "987          \n",
       "\n",
       "[988 rows x 4 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_text = [token.orth_ for token in doc]\n",
    "token_pos = [token.pos_ for token in doc]\n",
    "token_tag = [token.tag_ for token in doc]\n",
    "token_dep = [token.dep_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text,token_pos,token_tag,token_dep)), columns=['Text', 'POS','Tag','Dep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the parse tree\n",
    "SpaCy uses the terms *head* and *child* to describe the words connected by a single arc in the dependency tree. The term *dep* is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of *.dep* is a hash value. You can get the string value with *.dep\\_*.\n",
    "- Text: The original token text.\n",
    "- Dep: The syntactic relation connecting child to head.\n",
    "- Head text: The original text of the token head.\n",
    "- Head POS: The part-of-speech tag of the token head.\n",
    "- Children: The immediate syntactic dependents of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Head text</th>\n",
       "      <th>Head POS</th>\n",
       "      <th>Children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerró</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Cerró</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[comedor, ytenía, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>det</td>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedor</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Cerró</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[el, cordobés]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>amod</td>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[prometido]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>case</td>\n",
       "      <td>que</td>\n",
       "      <td>PRON</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>que</td>\n",
       "      <td>obj</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[al]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Macri</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le</td>\n",
       "      <td>obj</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>había</td>\n",
       "      <td>aux</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prometido</td>\n",
       "      <td>acl</td>\n",
       "      <td>cordobés</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[que, Macri, le, había, ayuda, Luis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayuda</td>\n",
       "      <td>obj</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Luis</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>prometido</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[Almadaes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Almadaes</td>\n",
       "      <td>flat</td>\n",
       "      <td>Luis</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>ytenía</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ytenía</td>\n",
       "      <td>advcl</td>\n",
       "      <td>Cerró</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[cordobés, comedor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td>det</td>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedor</td>\n",
       "      <td>obj</td>\n",
       "      <td>ytenía</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[un, comunitario, fundación]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comunitario</td>\n",
       "      <td>amod</td>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td>cc</td>\n",
       "      <td>fundación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>una</td>\n",
       "      <td>det</td>\n",
       "      <td>fundación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fundación</td>\n",
       "      <td>conj</td>\n",
       "      <td>comedor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[y, una, ayudar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>para</td>\n",
       "      <td>mark</td>\n",
       "      <td>ayudar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ayudar</td>\n",
       "      <td>acl</td>\n",
       "      <td>fundación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[para, gente, situación]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>case</td>\n",
       "      <td>gente</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gente</td>\n",
       "      <td>obj</td>\n",
       "      <td>ayudar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>case</td>\n",
       "      <td>situación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>situación</td>\n",
       "      <td>obl</td>\n",
       "      <td>ayudar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[en, calle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>de</td>\n",
       "      <td>case</td>\n",
       "      <td>calle</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>calle</td>\n",
       "      <td>nmod</td>\n",
       "      <td>situación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>Cerró</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>gobierno</td>\n",
       "      <td>nmod</td>\n",
       "      <td>conjunto</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[con, el, nacional]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>nacional</td>\n",
       "      <td>amod</td>\n",
       "      <td>gobierno</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>conjunto</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>retomaremos</td>\n",
       "      <td>advcl</td>\n",
       "      <td>Ratificamos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[conjunto, trabajo, sacar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>el</td>\n",
       "      <td>det</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>obj</td>\n",
       "      <td>retomaremos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[el, juntos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>juntos</td>\n",
       "      <td>amod</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>para</td>\n",
       "      <td>mark</td>\n",
       "      <td>sacar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>sacar</td>\n",
       "      <td>advcl</td>\n",
       "      <td>retomaremos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[para, personas, vivir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>a</td>\n",
       "      <td>case</td>\n",
       "      <td>personas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>todas</td>\n",
       "      <td>det</td>\n",
       "      <td>las</td>\n",
       "      <td>DET</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>las</td>\n",
       "      <td>det</td>\n",
       "      <td>personas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[todas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>personas</td>\n",
       "      <td>obj</td>\n",
       "      <td>sacar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[a, las, podamos, formación]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>que</td>\n",
       "      <td>obj</td>\n",
       "      <td>podamos</td>\n",
       "      <td>AUX</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>podamos</td>\n",
       "      <td>acl</td>\n",
       "      <td>personas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[que, calle, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>de</td>\n",
       "      <td>case</td>\n",
       "      <td>calle</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>la</td>\n",
       "      <td>det</td>\n",
       "      <td>calle</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>calle</td>\n",
       "      <td>obj</td>\n",
       "      <td>podamos</td>\n",
       "      <td>AUX</td>\n",
       "      <td>[de, la]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>podamos</td>\n",
       "      <td>AUX</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>darles</td>\n",
       "      <td>amod</td>\n",
       "      <td>formación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>formación</td>\n",
       "      <td>appos</td>\n",
       "      <td>personas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[darles, trabajo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>y</td>\n",
       "      <td>cc</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>conj</td>\n",
       "      <td>formación</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>[y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>para</td>\n",
       "      <td>mark</td>\n",
       "      <td>vivir</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>que</td>\n",
       "      <td>mark</td>\n",
       "      <td>vivir</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>puedan</td>\n",
       "      <td>aux</td>\n",
       "      <td>vivir</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>vivir</td>\n",
       "      <td>advcl</td>\n",
       "      <td>sacar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[para, que, puedan, dignamente]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dignamente</td>\n",
       "      <td>advmod</td>\n",
       "      <td>vivir</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>Ratificamos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[\\n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Text     Dep    Head text Head POS  \\\n",
       "0          Cerró    ROOT        Cerró     VERB   \n",
       "1             el     det      comedor     NOUN   \n",
       "2        comedor   nsubj        Cerró     VERB   \n",
       "3       cordobés    amod      comedor     NOUN   \n",
       "4             al    case          que     PRON   \n",
       "5            que     obj    prometido     VERB   \n",
       "6          Macri   nsubj    prometido     VERB   \n",
       "7             le     obj    prometido     VERB   \n",
       "8          había     aux    prometido     VERB   \n",
       "9      prometido     acl     cordobés      ADJ   \n",
       "10         ayuda     obj    prometido     VERB   \n",
       "11          Luis   nsubj    prometido     VERB   \n",
       "12      Almadaes    flat         Luis    PROPN   \n",
       "13      cordobés   nsubj       ytenía     VERB   \n",
       "14        ytenía   advcl        Cerró     VERB   \n",
       "15            un     det      comedor     NOUN   \n",
       "16       comedor     obj       ytenía     VERB   \n",
       "17   comunitario    amod      comedor     NOUN   \n",
       "18             y      cc    fundación     NOUN   \n",
       "19           una     det    fundación     NOUN   \n",
       "20     fundación    conj      comedor     NOUN   \n",
       "21          para    mark       ayudar     VERB   \n",
       "22        ayudar     acl    fundación     NOUN   \n",
       "23             a    case        gente     NOUN   \n",
       "24         gente     obj       ayudar     VERB   \n",
       "25            en    case    situación     NOUN   \n",
       "26     situación     obl       ayudar     VERB   \n",
       "27            de    case        calle     NOUN   \n",
       "28         calle    nmod    situación     NOUN   \n",
       "29             .   punct        Cerró     VERB   \n",
       "..           ...     ...          ...      ...   \n",
       "958     gobierno    nmod     conjunto     NOUN   \n",
       "959     nacional    amod     gobierno     NOUN   \n",
       "960            ,   punct     conjunto     NOUN   \n",
       "961  retomaremos   advcl  Ratificamos     VERB   \n",
       "962           el     det      trabajo     NOUN   \n",
       "963      trabajo     obj  retomaremos     VERB   \n",
       "964       juntos    amod      trabajo     NOUN   \n",
       "965         para    mark        sacar     VERB   \n",
       "966        sacar   advcl  retomaremos     VERB   \n",
       "967            a    case     personas     NOUN   \n",
       "968        todas     det          las      DET   \n",
       "969          las     det     personas     NOUN   \n",
       "970     personas     obj        sacar     VERB   \n",
       "971          que     obj      podamos      AUX   \n",
       "972      podamos     acl     personas     NOUN   \n",
       "973           de    case        calle     NOUN   \n",
       "974           la     det        calle     NOUN   \n",
       "975        calle     obj      podamos      AUX   \n",
       "976            ,   punct      podamos      AUX   \n",
       "977       darles    amod    formación     NOUN   \n",
       "978    formación   appos     personas     NOUN   \n",
       "979            y      cc      trabajo     NOUN   \n",
       "980      trabajo    conj    formación     NOUN   \n",
       "981         para    mark        vivir     VERB   \n",
       "982          que    mark        vivir     VERB   \n",
       "983       puedan     aux        vivir     VERB   \n",
       "984        vivir   advcl        sacar     VERB   \n",
       "985   dignamente  advmod        vivir     VERB   \n",
       "986            .   punct  Ratificamos     VERB   \n",
       "987           \\n                    .    PUNCT   \n",
       "\n",
       "                                 Children  \n",
       "0                    [comedor, ytenía, .]  \n",
       "1                                      []  \n",
       "2                          [el, cordobés]  \n",
       "3                             [prometido]  \n",
       "4                                      []  \n",
       "5                                    [al]  \n",
       "6                                      []  \n",
       "7                                      []  \n",
       "8                                      []  \n",
       "9    [que, Macri, le, había, ayuda, Luis]  \n",
       "10                                     []  \n",
       "11                             [Almadaes]  \n",
       "12                                     []  \n",
       "13                                     []  \n",
       "14                    [cordobés, comedor]  \n",
       "15                                     []  \n",
       "16           [un, comunitario, fundación]  \n",
       "17                                     []  \n",
       "18                                     []  \n",
       "19                                     []  \n",
       "20                       [y, una, ayudar]  \n",
       "21                                     []  \n",
       "22               [para, gente, situación]  \n",
       "23                                     []  \n",
       "24                                    [a]  \n",
       "25                                     []  \n",
       "26                            [en, calle]  \n",
       "27                                     []  \n",
       "28                                   [de]  \n",
       "29                                     []  \n",
       "..                                    ...  \n",
       "958                   [con, el, nacional]  \n",
       "959                                    []  \n",
       "960                                    []  \n",
       "961            [conjunto, trabajo, sacar]  \n",
       "962                                    []  \n",
       "963                          [el, juntos]  \n",
       "964                                    []  \n",
       "965                                    []  \n",
       "966               [para, personas, vivir]  \n",
       "967                                    []  \n",
       "968                                    []  \n",
       "969                               [todas]  \n",
       "970          [a, las, podamos, formación]  \n",
       "971                                    []  \n",
       "972                       [que, calle, ,]  \n",
       "973                                    []  \n",
       "974                                    []  \n",
       "975                              [de, la]  \n",
       "976                                    []  \n",
       "977                                    []  \n",
       "978                     [darles, trabajo]  \n",
       "979                                    []  \n",
       "980                                   [y]  \n",
       "981                                    []  \n",
       "982                                    []  \n",
       "983                                    []  \n",
       "984       [para, que, puedan, dignamente]  \n",
       "985                                    []  \n",
       "986                                  [\\n]  \n",
       "987                                    []  \n",
       "\n",
       "[988 rows x 5 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.text for token in doc]\n",
    "token_head_pos = [token.head.pos_ for token in doc]\n",
    "token_head_text = [token.head.text for token in doc]\n",
    "token_dep = [token.dep_ for token in doc]\n",
    "token_children = [[child for child in token.children] for token in doc]\n",
    "pd.DataFrame(list(zip(token_text,token_dep,token_head_text,token_head_pos,token_children)), columns=['Text','Dep','Head text','Head POS','Children'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Macri - PER\n",
      "Entity 2: Luis Almadaes - PER\n",
      "Entity 3: Desesperado - LOC\n",
      "Entity 4: Mauricio Macri - PER\n",
      "Entity 5: Allí - PER\n",
      "Entity 6: Yo Te Ayudo Amigo del Alma - MISC\n",
      "Entity 7: Almada - LOC\n",
      "Entity 8: La Naciónque - LOC\n",
      "Entity 9: Puse - PER\n",
      "Entity 10: No llamé al Presidente - MISC\n",
      "Entity 11: Macri me dijo - MISC\n",
      "Entity 12: Los tiempos de Nación - MISC\n",
      "Entity 13: Puse - PER\n",
      "Entity 14: No pedí nada para mí - MISC\n",
      "Entity 15: Vino el Presidente - PER\n",
      "Entity 16: Almada - LOC\n",
      "Entity 17: Provincia - LOC\n",
      "Entity 18: Municipalidad - LOC\n",
      "Entity 19: El proyecto quedó rengo - MISC\n",
      "Entity 20: Hace - ORG\n",
      "Entity 21: Carolina Stanley - PER\n",
      "Entity 22: Presidente - PER\n",
      "Entity 23: Ahora - MISC\n",
      "Entity 24: Almada - LOC\n",
      "Entity 25: Cómo - MISC\n",
      "Entity 26: No tengo más plata - MISC\n",
      "Entity 27: Laburo - MISC\n",
      "Entity 28: El día de la visita presidencial - MISC\n",
      "Entity 29: Talleres - ORG\n",
      "Entity 30: Sin embargo - MISC\n",
      "Entity 31: Almada - LOC\n",
      "Entity 32: Estado Nacional - LOC\n",
      "Entity 33: Ayer - LOC\n",
      "Entity 34: Ello nos obliga a seguirlos - MISC\n",
      "Entity 35: Yo te ayudo Amigo del Alma - MISC\n",
      "Entity 36: Estado Nacional - LOC\n",
      "Entity 37: Córdoba - LOC\n",
      "Entity 38: Ayer - LOC\n",
      "Entity 39: Ello nos obliga a seguirlos - MISC\n",
      "Entity 40: Cuando hablamos con el gobierno - MISC\n",
      "Entity 41: El Ministerio de Desarrollo Social de la Nación - MISC\n",
      "Entity 42: Mientras no los tuvimos nosotros salimos por nuestra - MISC\n",
      "Entity 43: Le dimos Comida - MISC\n",
      "Entity 44: Le dimos Trabajo - MISC\n",
      "Entity 45: Le dimos Albergue - MISC\n",
      "Entity 46: Este flagelo esta contaminando gravemente nuestra población - MISC\n",
      "Entity 47: Entendimos que no alcanza con alimentarlos - MISC\n",
      "Entity 48: Ayer - LOC\n",
      "Entity 49: Le - LOC\n",
      "Entity 50: Acordamos que estos meses sirvieron para fortalecer el conocimiento compartido - MISC\n",
      "Entity 51: El gobierno nos acompaña - MISC\n",
      "Entity 52: Tanto nosotros - MISC\n",
      "Entity 53: Estado entendemos - LOC\n",
      "Entity 54: Entendemos - MISC\n",
      "Entity 55: Estado - LOC\n",
      "Entity 56: Nosotros - MISC\n",
      "Entity 57: Desde el Estado - MISC\n",
      "Entity 58: Por eso es necesario - MISC\n",
      "Entity 59: ES GENTE BUENA Y TODOS TENEMOS - MISC\n",
      "Entity 60: AYUDARLOS - LOC\n",
      "Entity 61: ESTAN SOLOS - MISC\n",
      "Entity 62: DESPROTEGIDOS - MISC\n",
      "Entity 63: Ratificamos - LOC\n",
      "Entity 64: \n",
      " - MISC\n"
     ]
    }
   ],
   "source": [
    "for num, ent in enumerate(doc.ents):\n",
    "    print ('Entity {}:'.format(num + 1),ent,'-', ent.label_,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with displaCy\n",
    "SpaCy has an integrated visualization library that can display the content in two styles: *dep* and *ent*.\n",
    "The *dep* style shows the dependency between words using arcs, the *ent* style prints out the text with colored NER labels wrapped around words.\n",
    "\n",
    "The method *.serve()* launches a local web server for visualization while the method *.render()* generates an image.\n",
    "\n",
    "__Note__: Style *dep* is not working well in Spanish because *tag* is used instead of *POS* for annotating words, but the *tag* field is much larger than *POS* thus causing overlapping. \n",
    "\n",
    "__Note2__: Style *ent* can't be viewed in Github, but in Jupyter is great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "#displacy.serve(doc, style='dep')\n",
    "options = {'distance':425, 'arrow_spacing':6}\n",
    "displacy.render(doc,style='dep', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.serve(doc,style='dep')\n",
    "displacy.render(doc,style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text normalization: stemming, lemmatization and shape analysis\n",
    "Let's now move on to single token analysis. *Normalization* is a way of processing text that involves changing the words to make them less unique. We talk about *stemming* when we take the words and we remove the end part, producing a new token that often is not in the language dictionary. *Lemmatization* takes inflected words as input and tries to give the root word as output, so in some way is similar to stemming, but it produces meaningful (actually existing) words. The token *shape* is the de-capitalization char mask that gets applied to the original (orthodox) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerró</td>\n",
       "      <td>cerró</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedor</td>\n",
       "      <td>comedor</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>cordobés</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>al</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Macri</td>\n",
       "      <td>macri</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le</td>\n",
       "      <td>le</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>había</td>\n",
       "      <td>había</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prometido</td>\n",
       "      <td>prometido</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayuda</td>\n",
       "      <td>ayuda</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Luis</td>\n",
       "      <td>luis</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Almadaes</td>\n",
       "      <td>almadaes</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>cordobés</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ytenía</td>\n",
       "      <td>ytenía</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedor</td>\n",
       "      <td>comedor</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comunitario</td>\n",
       "      <td>comunitario</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>una</td>\n",
       "      <td>una</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fundación</td>\n",
       "      <td>fundación</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>para</td>\n",
       "      <td>para</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ayudar</td>\n",
       "      <td>ayudar</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gente</td>\n",
       "      <td>gente</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>situación</td>\n",
       "      <td>situación</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>calle</td>\n",
       "      <td>calle</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>gobierno</td>\n",
       "      <td>gobierno</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>nacional</td>\n",
       "      <td>nacional</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>retomaremos</td>\n",
       "      <td>retomaremos</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>juntos</td>\n",
       "      <td>juntos</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>para</td>\n",
       "      <td>para</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>sacar</td>\n",
       "      <td>sacar</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>todas</td>\n",
       "      <td>todas</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>las</td>\n",
       "      <td>las</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>personas</td>\n",
       "      <td>personas</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>podamos</td>\n",
       "      <td>podamos</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>la</td>\n",
       "      <td>la</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>calle</td>\n",
       "      <td>calle</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>darles</td>\n",
       "      <td>darles</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>formación</td>\n",
       "      <td>formación</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>para</td>\n",
       "      <td>para</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>puedan</td>\n",
       "      <td>puedan</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>vivir</td>\n",
       "      <td>vivir</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dignamente</td>\n",
       "      <td>dignamente</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text  token_lemma token_shape\n",
       "0          Cerró        cerró       Xxxxx\n",
       "1             el           el          xx\n",
       "2        comedor      comedor        xxxx\n",
       "3       cordobés     cordobés        xxxx\n",
       "4             al           al          xx\n",
       "5            que          que         xxx\n",
       "6          Macri        macri       Xxxxx\n",
       "7             le           le          xx\n",
       "8          había        había        xxxx\n",
       "9      prometido    prometido        xxxx\n",
       "10         ayuda        ayuda        xxxx\n",
       "11          Luis         luis        Xxxx\n",
       "12      Almadaes     almadaes       Xxxxx\n",
       "13      cordobés     cordobés        xxxx\n",
       "14        ytenía       ytenía        xxxx\n",
       "15            un           un          xx\n",
       "16       comedor      comedor        xxxx\n",
       "17   comunitario  comunitario        xxxx\n",
       "18             y            y           x\n",
       "19           una          una         xxx\n",
       "20     fundación    fundación        xxxx\n",
       "21          para         para        xxxx\n",
       "22        ayudar       ayudar        xxxx\n",
       "23             a            a           x\n",
       "24         gente        gente        xxxx\n",
       "25            en           en          xx\n",
       "26     situación    situación        xxxx\n",
       "27            de           de          xx\n",
       "28         calle        calle        xxxx\n",
       "29             .            .           .\n",
       "..           ...          ...         ...\n",
       "958     gobierno     gobierno        xxxx\n",
       "959     nacional     nacional        xxxx\n",
       "960            ,            ,           ,\n",
       "961  retomaremos  retomaremos        xxxx\n",
       "962           el           el          xx\n",
       "963      trabajo      trabajo        xxxx\n",
       "964       juntos       juntos        xxxx\n",
       "965         para         para        xxxx\n",
       "966        sacar        sacar        xxxx\n",
       "967            a            a           x\n",
       "968        todas        todas        xxxx\n",
       "969          las          las         xxx\n",
       "970     personas     personas        xxxx\n",
       "971          que          que         xxx\n",
       "972      podamos      podamos        xxxx\n",
       "973           de           de          xx\n",
       "974           la           la          xx\n",
       "975        calle        calle        xxxx\n",
       "976            ,            ,           ,\n",
       "977       darles       darles        xxxx\n",
       "978    formación    formación        xxxx\n",
       "979            y            y           x\n",
       "980      trabajo      trabajo        xxxx\n",
       "981         para         para        xxxx\n",
       "982          que          que         xxx\n",
       "983       puedan       puedan        xxxx\n",
       "984        vivir        vivir        xxxx\n",
       "985   dignamente   dignamente        xxxx\n",
       "986            .            .           .\n",
       "987           \\n           \\n          \\n\n",
       "\n",
       "[988 rows x 3 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in doc]\n",
    "token_shape = [token.shape_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too bad, lemmatization is actually not supported for the Spanish language model (for the English model however it has good support). We still have some normalization, as seen from the shape mask applied to every word.\n",
    "\n",
    "### Token-level entity analysis\n",
    "The standard way to access entity annotations is the *doc.ents* property, but you can also access token entity annotations using the *token.ent_iob* and *token.ent_type* attributes; *token.ent_iob* indicates whether an entity starts, continues or ends on the tag.\n",
    "\n",
    "IOB Scheme:\n",
    "- *I* - Token is *inside* an entity.\n",
    "- *O* - Token is *outside* an entity.\n",
    "- *B* - Token is the *beginning* of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerró</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedor</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cordobés</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>que</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Macri</td>\n",
       "      <td>PER</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>había</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prometido</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayuda</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Luis</td>\n",
       "      <td>PER</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Almadaes</td>\n",
       "      <td>PER</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cordobés</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ytenía</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedor</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comunitario</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>una</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fundación</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>para</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ayudar</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gente</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>situación</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>calle</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>gobierno</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>nacional</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>retomaremos</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>el</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>trabajo</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>juntos</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>para</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>sacar</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>todas</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>las</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>personas</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>que</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>podamos</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>la</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>calle</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>darles</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>formación</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>y</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>trabajo</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>para</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>que</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>puedan</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>vivir</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dignamente</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>\\n</td>\n",
       "      <td>MISC</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text entity_type inside_outside_begin\n",
       "0          Cerró                                O\n",
       "1             el                                O\n",
       "2        comedor                                O\n",
       "3       cordobés                                O\n",
       "4             al                                O\n",
       "5            que                                O\n",
       "6          Macri         PER                    B\n",
       "7             le                                O\n",
       "8          había                                O\n",
       "9      prometido                                O\n",
       "10         ayuda                                O\n",
       "11          Luis         PER                    B\n",
       "12      Almadaes         PER                    I\n",
       "13      cordobés                                O\n",
       "14        ytenía                                O\n",
       "15            un                                O\n",
       "16       comedor                                O\n",
       "17   comunitario                                O\n",
       "18             y                                O\n",
       "19           una                                O\n",
       "20     fundación                                O\n",
       "21          para                                O\n",
       "22        ayudar                                O\n",
       "23             a                                O\n",
       "24         gente                                O\n",
       "25            en                                O\n",
       "26     situación                                O\n",
       "27            de                                O\n",
       "28         calle                                O\n",
       "29             .                                O\n",
       "..           ...         ...                  ...\n",
       "958     gobierno                                O\n",
       "959     nacional                                O\n",
       "960            ,                                O\n",
       "961  retomaremos                                O\n",
       "962           el                                O\n",
       "963      trabajo                                O\n",
       "964       juntos                                O\n",
       "965         para                                O\n",
       "966        sacar                                O\n",
       "967            a                                O\n",
       "968        todas                                O\n",
       "969          las                                O\n",
       "970     personas                                O\n",
       "971          que                                O\n",
       "972      podamos                                O\n",
       "973           de                                O\n",
       "974           la                                O\n",
       "975        calle                                O\n",
       "976            ,                                O\n",
       "977       darles                                O\n",
       "978    formación                                O\n",
       "979            y                                O\n",
       "980      trabajo                                O\n",
       "981         para                                O\n",
       "982          que                                O\n",
       "983       puedan                                O\n",
       "984        vivir                                O\n",
       "985   dignamente                                O\n",
       "986            .                                O\n",
       "987           \\n        MISC                    B\n",
       "\n",
       "[988 rows x 3 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in doc]\n",
    "token_entity_iob = [token.ent_iob_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)), columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level attributes\n",
    "Other useful metadata is provided, such as the relative frequency of tokens, and whether or not a token matches any of these categories:\n",
    "- stop-word\n",
    "- punctuation\n",
    "- whitespace\n",
    "- number\n",
    "- url\n",
    "\n",
    "...and many more token [attributes](https://alpha.spacy.io/api/token#attributes)! If you are using the alpha version of spaCy, you can also add [custom attributes](https://explosion.ai/blog/spacy-v2-pipelines-extensions) to tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prob</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>url?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerró</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedor</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>que</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Macri</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>había</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prometido</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayuda</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Luis</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Almadaes</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cordobés</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ytenía</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedor</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comunitario</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>una</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fundación</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>para</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ayudar</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gente</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>situación</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>de</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>calle</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>gobierno</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>nacional</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>retomaremos</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>el</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>juntos</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>para</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>sacar</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>todas</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>las</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>personas</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>que</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>podamos</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>de</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>la</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>calle</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>darles</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>formación</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>y</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>para</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>que</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>puedan</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>vivir</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dignamente</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  prob stop? punctuation? whitespace? number? url?\n",
       "0          Cerró -20.0                                            \n",
       "1             el -20.0   Yes                                      \n",
       "2        comedor -20.0                                            \n",
       "3       cordobés -20.0                                            \n",
       "4             al -20.0   Yes                                      \n",
       "5            que -20.0   Yes                                      \n",
       "6          Macri -20.0                                            \n",
       "7             le -20.0   Yes                                      \n",
       "8          había -20.0   Yes                                      \n",
       "9      prometido -20.0                                            \n",
       "10         ayuda -20.0                                            \n",
       "11          Luis -20.0                                            \n",
       "12      Almadaes -20.0                                            \n",
       "13      cordobés -20.0                                            \n",
       "14        ytenía -20.0                                            \n",
       "15            un -20.0   Yes                                      \n",
       "16       comedor -20.0                                            \n",
       "17   comunitario -20.0                                            \n",
       "18             y -20.0                                            \n",
       "19           una -20.0   Yes                                      \n",
       "20     fundación -20.0                                            \n",
       "21          para -20.0   Yes                                      \n",
       "22        ayudar -20.0                                            \n",
       "23             a -20.0                                            \n",
       "24         gente -20.0                                            \n",
       "25            en -20.0   Yes                                      \n",
       "26     situación -20.0                                            \n",
       "27            de -20.0   Yes                                      \n",
       "28         calle -20.0                                            \n",
       "29             . -20.0                Yes                         \n",
       "..           ...   ...   ...          ...         ...     ...  ...\n",
       "958     gobierno -20.0                                            \n",
       "959     nacional -20.0                                            \n",
       "960            , -20.0                Yes                         \n",
       "961  retomaremos -20.0                                            \n",
       "962           el -20.0   Yes                                      \n",
       "963      trabajo -20.0   Yes                                      \n",
       "964       juntos -20.0                                            \n",
       "965         para -20.0   Yes                                      \n",
       "966        sacar -20.0                                            \n",
       "967            a -20.0                                            \n",
       "968        todas -20.0   Yes                                      \n",
       "969          las -20.0   Yes                                      \n",
       "970     personas -20.0                                            \n",
       "971          que -20.0   Yes                                      \n",
       "972      podamos -20.0                                            \n",
       "973           de -20.0   Yes                                      \n",
       "974           la -20.0   Yes                                      \n",
       "975        calle -20.0                                            \n",
       "976            , -20.0                Yes                         \n",
       "977       darles -20.0                                            \n",
       "978    formación -20.0                                            \n",
       "979            y -20.0                                            \n",
       "980      trabajo -20.0   Yes                                      \n",
       "981         para -20.0   Yes                                      \n",
       "982          que -20.0   Yes                                      \n",
       "983       puedan -20.0                                            \n",
       "984        vivir -20.0                                            \n",
       "985   dignamente -20.0                                            \n",
       "986            . -20.0                Yes                         \n",
       "987           \\n -20.0                            Yes             \n",
       "\n",
       "[988 rows x 7 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,token.prob,token.is_stop,token.is_punct,token.is_space,token.like_num,token.like_url) for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,columns=['text','prob','stop?','punctuation?','whitespace?','number?','url?'])\n",
    "\n",
    "df.loc[:, 'stop?':'url?'] = (df.loc[:, 'stop?':'url?'].applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative frequency is not stored in the model, but that's not important since we don't intend to rely on it anyways.  We can see that there are some problems with stop-words, for example \"trabajo\" should not be considered a stop-word, thus in the next section we have to manually adjust this attribute.\n",
    "\n",
    "## Text normalization, lemmatization, stop-words removal and sentence segmentation\n",
    "Now that we have explored all that spaCy can do for us, we can use it to parse our *text.txt* and generate a new *parsed_text.txt* that has the same text, normalized, lemmatized, deprived of stop-words and segmented in sentences.\n",
    "\n",
    "We first define a helper function that constructs a generator to loop over the *text.txt* and yield the posts one-by-one. A generator is similar to an iterator but it can be used only once because its content is generated on the fly and not stored in memory, saving precious computation.\n",
    "\n",
    "Then, we pass on the posts to spaCy using the *.pipe()* method via a generator function to parse the posts, lemmatize the text, and yield segmantized sentences. The standard way to initialize spaCy would be to call *nlp(text.txt)* on each post, but I will make use instead of the *.pipe()* method which allows efficient [multi-threading](https://spacy.io/docs/usage/processing-text#multithreading). Two [arguments](https://alpha.spacy.io/api/language#pipe) are given to *.pipe()*: *batch_size* is the number of posts to buffer and *n_threads* which is the number of worker threads to use (default is 2, if -1 OpenMP will decide how many to use at run time). You can also pass a *disable* option to turn off some components of the pipeline that is not needed to further optimize the processing. Note that all processing algorithms are linear-time in the length of the string. \n",
    "\n",
    "Luckily for us, spaCy makes it really easy to modify the pipeline. As explained in the former section, we are going to insert some custom stop-words that were missing in our vocabulary and remove some other ones. As you can see some tokens should be considered stop-words, such as \"y\" and \"a\" are not correctly identified, and, viceversa, words such as \"trabajo\" should not be classified as stop-words. To fix this, we have to list all stop-words present in our model and if they are not supposed to be stop-words we can manually remove them from the pipeline, while to include new stop-words we just have to see if they appear in our topic models and only then come back and label them as stop-words. Another thing we will add to spaCy's pipeline is a custom normalization that replaces accent characters such as *è* and *é* with regular characters such as a simpler *e*, because some people choose to use accents and some don't, and in topic modeling we don't want to have two separate entries for *macrì* and *macri*. We also fix some punctuation although it will be removed anyway by the lemmatizer.\n",
    "\n",
    "Finally, we write the sentences to a new txt file, *parsed_text.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 66046 total stop words.\n",
      "There are 607 unique stop words.\n",
      "{'cerca', 'Dos', 'Despues', 'habla', 'conocer', 'manera', 'EL', 'Muchos', 'algunos', 'Las', 'pueda', 'QUE', 'ellas', 'largo', 'tu', 'esta', 'conmigo', 'Nunca', 'Sera', 'Ahi', 'mientras', 'Fueron', 'Ellos', 'NO', 'Dio', 'aproximadamente', 'un', 'tienen', 'propias', 'Les', 'menos', 'mayor', 'alli', 'eso', 'Nada', 'existen', 'emplean', 'aquellos', 'cual', 'Conseguir', 'Ahora', 'Tengo', 'Se', 'Aun', 'su', 'A', 'TODO', 'veces', 'este', 'pues', 'lleva', 'tres', 'Tambien', 'Nuestro', 'Fuera', 'Tres', 'fui', 'Ademas', 'ahi', 'Enseguida', 'Poder', 'cuatro', 'En', 'dan', 'unas', 'Desde', 'Hizo', 'USO', 'parece', 'eran', 'Yo', 'esa', 'arriba', 'e', 'eramos', 'Solamente', 'Podemos', 'los', 'nadie', 'parte', 'intentan', 'Tuvo', 'Todos', 'Creo', 'es', 'fue', 'otros', 'ninguna', 'varios', 'alguno', 'and', 'Tiene', 'dijeron', 'alguna', 'AuN', 'tampoco', 'nada', 'TODOS', 'Usted', 'Otras', 'tuvo', 'Hubo', 'Ustedes', 'sino', 'Toda', 'ciertos', 'vaya', 'somos', 'Mas', 'Nosotras', 'Tras', 'no', 'intentar', 'aquellas', 'hubo', 'Esa', 'final', 'saber', 'Algunas', 'Cuales', 'Bien', 'Solo', 'ALGUNA', 'DEL', 'Uno', 'informo', 'mio', 'Podrias', 'podriamos', 'sola', 'demasiado', 'partir', 'Apenas', 'Ese', 'pronto', 'Gran', 'muchas', 'ver', 'Cualquier', 'quiere', 'Hace', 'encima', 'sin', 'todas', 'ultimo', 'propia', 'DOS', 'mi', 'Sus', 'Aunque', 'han', 'para', 'tarde', 'POR', 'otras', 'Habla', 'tambien', 'Somos', 'ocho', 'CON', 'Podria', 'Saben', 'en', 'quizas', 'vamos', 'Mis', 'Cuando', 'Sabes', 'asi', 'Muchas', 'Ha', 'sera', 'Mientras', 'dicho', 'bastante', 'cuenta', 'tenga', 'Antes', 'cinco', 'DADO', 'Nuestra', 'estaba', 'UN', 'supuesto', 'pudo', 'te', 'LOS', 'SIN', 'despues', 'pais', 'ante', 'creo', 'The', 'poner', 'todo', 'cualquier', 'ha', 'dos', 'Esto', 'cierta', 'Hemos', 'dice', 'Soy', 'SE', 'enfrente', 'ese', 'Con', 'les', 'alrededor', 'O', 'Fue', 'NOS', 'cuales', 'dentro', 'Para', 'Ni', 'Va', 'ciertas', 'estar', 'Algunos', 'algunas', 'esto', 'o', 'mis', 'conseguir', 'Estan', 'de', 'Nadie', 'segun', 'toda', 'Esas', 'yo', 'muy', 'trata', 'Hoy', 'SOBRE', 'Claro', 'Estoy', 'fuera', 'Fui', 'Hacia', 'ademas', 'eras', 'estamos', 'ser', 'hacia', 'Incluso', 'Sin', 'solo', 'Vamos', 'horas', 'tuyos', 'Estas', 'Eso', 'que', 'Por', 'Han', 'decir', 'Actualmente', 'Alguna', 'ir', 'Otro', 'entonces', 'si', 'usar', 'Los', 'Cual', 'Voy', 'Cosas', 'Existe', 'considera', 'Lo', 'suyo', 'tiene', 'haciendo', 'Es', 'ex', 'y', 'hace', 'Debe', 'bajo', 'una', 'ESA', 'Aqui', 'Dicen', 'tenemos', 'aqui', 'todos', 'Ninguna', 'Pues', 'ciento', 'TODAS', 'TENEMOS', 'ya', 'Horas', 'apenas', 'SOLOS', 'menudo', 'total', 'mucho', 'siguiente', 'AL', 'hicieron', 'Estaba', 'Mucha', 'junto', 'suya', 'cierto', 'existe', 'debajo', 'ello', 'Usamos', 'Son', 'uno', 'Saber', 'principalmente', 'ellos', 'Segun', 'seis', 'esas', 'ES', 'LA', 'modo', 'nos', 'Cada', 'DURANTE', 'usted', 'las', 'CADA', 'siempre', 'incluso', 'Ya', 'la', 'Cuatro', 'Sean', 'solos', 'lo', 'tener', 'Tampoco', 'entre', 'podrian', 'dieron', 'Habia', 'cuanto', 'Ninguno', 'ni', 'mas', 'the', 'Esta', 'DESDE', 'todavia', 'cuantos', 'Dias', 'nuestra', 'Ser', 'podria', 'De', 'hoy', 'aunque', 'fin', 'otro', 'dio', 'donde', 'LE', 'mediante', 'años', 'Del', 'anterior', 'Estuvo', 'saben', 'Ambos', 'sido', 'uso', 'Ante', 'Mi', 'unos', 'Su', 'fueron', 'ambos', 'consigo', 'muchos', 'estaban', 'Queremos', 'Existen', 'mucha', 'era', 'Estamos', 'Y', 'sea', 'E', 'desde', 'Ello', 'mia', 'Dado', 'Algo', 'cosas', 'porque', 'son', 'algo', 'Salvo', 'aquello', 'da', 'Quizas', 'sus', 'DE', 'segundo', 'usan', 'Como', 'sabe', 'Tal', 'poca', 'temprano', 'siete', 'va', 'sean', 'nunca', 'adelante', 'LAS', 'Alli', 'No', 'esos', 'nuestros', 'PARA', 'UNA', 'del', 'hacerlo', 'Parece', 'casi', 'quienes', 'Le', 'encuentra', 'se', 'Quien', 'conseguimos', 'queremos', 'Muy', 'como', 'usa', 'Puede', 'propio', 'dia', 'ninguno', 'pesar', 'tan', 'tercera', 'sabes', 'Sobre', 'Ayer', 'buen', 'Dijo', 'soy', 'por', 'poder', 'al', 'salvo', 'podemos', 'Otra', 'vez', 'El', 'eres', 'propios', 'he', 'OTRAS', 'cada', 'usamos', 'le', 'Tenemos', 'Dentro', 'Este', 'Casi', 'dijo', 'ahora', 'estos', 'Pero', 'ESTAN', 'nuestro', 'nosotros', 'detras', 'año', 'tanto', 'estas', 'EN', 'pocas', 'hemos', 'haya', 'Considera', 'dar', 'Dan', 'lado', 'ayer', 'Era', 'contra', 'con', 'a', 'Detras', 'hecho', 'Entonces', 'sabemos', 'hacemos', 'Nos', 'durante', 'Una', 'enseguida', 'Ella', 'Siempre', 'Poner', 'Nosotros', 'Debajo', 'tal', 'tenido', 'Porque', 'trabajar', 'pero', 'puedo', 'actualmente', 'pueden', 'hacer', 'Contra', 'nosotras', 'Hacer', 'solamente', 'llevar', 'hacen', 'debe', 'podrias', 'Que', 'Al', 'LO', 'me', 'aun', 'Si', 'habia', 'claro', 'PUEDE', 'bien', 'And', 'dicen', 'van', 'Sigue', 'dias', 'Lejos', 'SINO', 'Te', 'Tu', 'Cuanto', 'fuimos', 'dado', 'aquella', 'peor', 'tras', 'Eran', 'hay', 'Tanto', 'estoy', 'sobre', 'sigue', 'gran', 'Todas', 'Otros', 'Todo', 'deben', 'Me', 'estan', 'Hacemos', 'Hay', 'Junto', 'antes', 'Dia', 'lejos', 'atras', 'cuanta', 'VER', 'tengo', 'hizo', 'tus', 'ella', 'ustedes', 'cuando', 'el', 'Mucho', 'intenta', 'Entre', 'haber', 'puede', 'voy', 'aquel', 'Estos', 'nuestras', 'quien', 'breve', 'Todavia', 'Quienes', 'La', 'Un', 'otra', 'Mia', 'contigo', 'siendo', 'estuvo', 'segunda', 'Fin', 'Durante', 'Donde', 'Ex', 'varias', 'delante', 'hablan'}\n",
      "CPU times: user 38.8 s, sys: 43.5 s, total: 1min 22s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Helper function that yields all posts via generator \n",
    "def get_post(filename):\n",
    "    with codecs.open(filename,encoding='utf-8') as textfile:\n",
    "        for post in textfile:\n",
    "            post = post.replace('ó','o')            \n",
    "            post = post.replace('ó','o')\n",
    "            post = post.replace('Ó','o')\n",
    "            post = post.replace('Ò','o')\n",
    "            post = post.replace('í','i')\n",
    "            post = post.replace('ì','i')\n",
    "            post = post.replace('Ì','i')            \n",
    "            post = post.replace('Í','i')            \n",
    "            post = post.replace('à','a')\n",
    "            post = post.replace('á','a')\n",
    "            post = post.replace('À','a')\n",
    "            post = post.replace('Á','a')\n",
    "            post = post.replace('ù','u')\n",
    "            post = post.replace('Ù','u')\n",
    "            post = post.replace('Ú','u')\n",
    "            post = post.replace('ú','u')\n",
    "            post = post.replace('ü','u')\n",
    "            post = post.replace('è','e')\n",
    "            post = post.replace('é','e')\n",
    "            post = post.replace('È','e')\n",
    "            post = post.replace('É','e')\n",
    "            post = post.replace('¿','')\n",
    "            post = post.replace('“','\\\"')\n",
    "            post = post.replace('”','\\\"') \n",
    "            post = post.replace('\\’','\\\"') \n",
    "            yield post\n",
    "            \n",
    "#Add and remove custom stop words\n",
    "nlp.vocab[\"A\"].is_stop = True\n",
    "nlp.vocab[\"a\"].is_stop = True\n",
    "nlp.vocab[\"Y\"].is_stop = True\n",
    "nlp.vocab[\"y\"].is_stop = True\n",
    "nlp.vocab[\"o\"].is_stop = True\n",
    "nlp.vocab[\"O\"].is_stop = True\n",
    "nlp.vocab[\"and\"].is_stop = True\n",
    "nlp.vocab[\"And\"].is_stop = True\n",
    "nlp.vocab[\"the\"].is_stop = True\n",
    "nlp.vocab[\"The\"].is_stop = True\n",
    "nlp.vocab[\"e\"].is_stop = True\n",
    "nlp.vocab[\"E\"].is_stop = True\n",
    "nlp.vocab[\"ciento\"].is_stop = True\n",
    "nlp.vocab[\"año\"].is_stop = True\n",
    "nlp.vocab[\"años\"].is_stop = True\n",
    "nlp.vocab[\"trabajo\"].is_stop = False\n",
    "nlp.vocab[\"Trabajo\"].is_stop = False\n",
    "nlp.vocab[\"Trabajar\"].is_stop = False\n",
    "nlp.vocab[\"trabajan\"].is_stop = False\n",
    "nlp.vocab[\"Trabaja\"].is_stop = False\n",
    "nlp.vocab[\"trabaja\"].is_stop = False\n",
    "nlp.vocab[\"tiempo\"].is_stop = False\n",
    "nlp.vocab[\"Tiempo\"].is_stop = False\n",
    "nlp.vocab[\"Respecto\"].is_stop = False\n",
    "nlp.vocab[\"respecto\"].is_stop = False\n",
    "nlp.vocab[\"primero\"].is_stop = False\n",
    "nlp.vocab[\"primera\"].is_stop = False\n",
    "nlp.vocab[\"PRIMERO\"].is_stop = False\n",
    "nlp.vocab[\"primeros\"].is_stop = False\n",
    "nlp.vocab[\"primer\"].is_stop = False\n",
    "nlp.vocab[\"Primero\"].is_stop = False\n",
    "nlp.vocab[\"Primera\"].is_stop = False\n",
    "nlp.vocab[\"Momento\"].is_stop = False\n",
    "nlp.vocab[\"momento\"].is_stop = False\n",
    "nlp.vocab[\"MOMENTO\"].is_stop = False\n",
    "nlp.vocab[\"Estado\"].is_stop = False\n",
    "nlp.vocab[\"estado\"].is_stop = False\n",
    "nlp.vocab[\"Estados\"].is_stop = False\n",
    "nlp.vocab[\"grandes\"].is_stop = False\n",
    "nlp.vocab[\"diferente\"].is_stop = False\n",
    "nlp.vocab[\"diferentes\"].is_stop = False\n",
    "nlp.vocab[\"realizar\"].is_stop = False\n",
    "nlp.vocab[\"realizado\"].is_stop = False\n",
    "nlp.vocab[\"REALIZAR\"].is_stop = False\n",
    "nlp.vocab[\"proximo\"].is_stop = False\n",
    "nlp.vocab[\"empleo\"].is_stop = False\n",
    "nlp.vocab[\"Empleo\"].is_stop = False\n",
    "nlp.vocab[\"acuerdo\"].is_stop = False\n",
    "nlp.vocab[\"pasado\"].is_stop = False\n",
    "nlp.vocab[\"pasada\"].is_stop = False\n",
    "nlp.vocab[\"Van\"].is_stop = False\n",
    "nlp.vocab[\"finally\"].is_stop = False\n",
    "nlp.vocab[\"General\"].is_stop = False\n",
    "nlp.vocab[\"general\"].is_stop = False\n",
    "nlp.vocab[\"Asi\"].is_stop = False\n",
    "nlp.vocab[\"misma\"].is_stop = False\n",
    "nlp.vocab[\"mismo\"].is_stop = False\n",
    "nlp.vocab[\"mismas\"].is_stop = False\n",
    "nlp.vocab[\"mismos\"].is_stop = False\n",
    "nlp.vocab[\"nuevo\"].is_stop = False\n",
    "nlp.vocab[\"nuevos\"].is_stop = False\n",
    "nlp.vocab[\"Nuevo\"].is_stop = False\n",
    "nlp.vocab[\"NUEVO\"].is_stop = False\n",
    "nlp.vocab[\"nuevas\"].is_stop = False\n",
    "nlp.vocab[\"Nueva\"].is_stop = False\n",
    "nlp.vocab[\"nueva\"].is_stop = False\n",
    "nlp.vocab[\"igual\"].is_stop = False\n",
    "nlp.vocab[\"Igual\"].is_stop = False\n",
    "nlp.vocab[\"Debido\"].is_stop = False\n",
    "nlp.vocab[\"debido\"].is_stop = False\n",
    "nlp.vocab[\"ejemplo\"].is_stop = False\n",
    "nlp.vocab[\"verdad\"].is_stop = False\n",
    "nlp.vocab[\"Verdad\"].is_stop = False\n",
    "nlp.vocab[\"valor\"].is_stop = False\n",
    "nlp.vocab[\"Valor\"].is_stop = False\n",
    "nlp.vocab[\"VALOR\"].is_stop = False\n",
    "nlp.vocab[\"HASTA\"].is_stop = False\n",
    "nlp.vocab[\"hasta\"].is_stop = False\n",
    "nlp.vocab[\"Hasta\"].is_stop = False\n",
    "nlp.vocab[\"Buenos\"].is_stop = False\n",
    "nlp.vocab[\"buenos\"].is_stop = False\n",
    "nlp.vocab[\"BUENOS\"].is_stop = False\n",
    "nlp.vocab[\"medio\"].is_stop = False\n",
    "nlp.vocab[\"Medio\"].is_stop = False\n",
    "nlp.vocab[\"lugar\"].is_stop = False\n",
    "nlp.vocab[\"mejor\"].is_stop = False\n",
    "nlp.vocab[\"buena\"].is_stop = False\n",
    "nlp.vocab[\"BUENA\"].is_stop = False\n",
    "nlp.vocab[\"Bueno\"].is_stop = False\n",
    "nlp.vocab[\"bueno\"].is_stop = False\n",
    "nlp.vocab[\"luego\"].is_stop = False\n",
    "nlp.vocab[\"Luego\"].is_stop = False\n",
    "nlp.vocab[\"mal\"].is_stop = False\n",
    "nlp.vocab[\"poco\"].is_stop = False\n",
    "nlp.vocab[\"Poco\"].is_stop = False\n",
    "nlp.vocab[\"Pocos\"].is_stop = False\n",
    "nlp.vocab[\"pocos\"].is_stop = False\n",
    "nlp.vocab[\"embargo\"].is_stop = False\n",
    "nlp.vocab[\"verdadero\"].is_stop = False\n",
    "nlp.vocab[\"verdadera\"].is_stop = False\n",
    "nlp.vocab[\"posible\"].is_stop = False\n",
    "nlp.vocab[\"intento\"].is_stop = False\n",
    "\n",
    "#List current stop words\n",
    "stop_words = []\n",
    "for parsed_post in nlp.pipe(get_post(\"data/text.txt\"),batch_size=10, n_threads=3):\n",
    "    for sent in parsed_post.sents:\n",
    "        for token in sent:\n",
    "            if token.is_stop:\n",
    "                stop_words.append(token.orth_)\n",
    "print(\"There are {} total stop words.\".format(len(stop_words)))\n",
    "stop_set = set(stop_words)\n",
    "print(\"There are {} unique stop words.\".format(len(stop_set)))\n",
    "print(stop_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to lemmatize our corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harvey', 'weinstein', 'caida', 'señor', 'hollywood', 'acusaciones', 'escandalosas', 'ashley', 'juddfue', 'primeras', 'denunciar', 'demoro', 'llego', 'hotel', 'hospedaba', 'weinstein', 'creyendo', 'asistia', 'reunion', 'negocios', 'productor', 'aparecio', 'bata', 'baño', 'pidio', 'masaje', 'viera', 'ducharse', 'gwyneth', 'paltrow', 'tenia', 'harvey', 'weinstein', 'manoseo', 'suite', 'hotel', 'beverly', 'hills', 'angelina', 'jolie', 'conto', 'avanzo', 'habitacion', 'hotel', 'rechazo', 'lucia', 'evansdijo', 'llegar', 'casting', 'weinstein', 'ataco', 'sexualmente', 'obligo', 'practicarle', 'sexo', 'oral', 'ambra', 'battilana', 'gutierrez', 'finalista', 'miss', 'italia', 'conto', 'weinstein', 'lanzo', 'toco', 'pechos', 'trato', 'meter', 'mano', 'minifalda', 'cara', 'delevingne', 'asegura', 'weinstein', 'pidio', 'trio', 'lea', 'seydoux', 'avalanzo', 'intento', 'besarla', 'katherine', 'kendallacompaño', 'productor', 'casa', 'llegaron', 'weinstein', 'desvistio', 'persiguio', 'desnudo', 'casa', 'asia', 'argentodenuncio', 'weinstein', 'quito', 'falda', 'forzo', 'practicar', 'sexo', 'oral', 'mira', 'sorvino', 'intento', 'sobrepasarse', 'actriz', 'rechazo', 'rosanna', 'arquette', 'conto', 'nego', 'masturbarlo']\n",
      "CPU times: user 1min 15s, sys: 1min 26s, total: 2min 41s\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Parse posts and lemmatize each token sentence by sentence. Then Reassemble the sentences for every post. \n",
    "#WARNING: This task is computationally demanding, adjust batch_size and n_threads according to your machine\n",
    "\n",
    "#All posts\n",
    "for parsed_post in nlp.pipe(get_post(\"data/text.txt\"),batch_size=10, n_threads=3):\n",
    "    with codecs.open('data/parsed_text.txt','a',encoding='utf-8') as f:\n",
    "        for sent in parsed_post.sents:\n",
    "            for token in sent:\n",
    "                if not (token.is_punct or token.is_space or token.is_stop or token.like_num or token.like_email or token.like_url):\n",
    "                    f.write(''.join(token.lemma_) + ' ')\n",
    "        f.write('\\n') \n",
    "#Phrases        \n",
    "    with codecs.open('data/parsed_phrases.txt','a',encoding='utf-8') as f:\n",
    "        for sent in parsed_post.sents:\n",
    "            for token in sent:\n",
    "                if not (token.is_punct or token.is_space or token.is_stop or token.like_num or token.like_email or token.like_url):\n",
    "                    f.write(''.join(token.lemma_) + ' ')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "   \n",
    "\n",
    "nest = []\n",
    "for parsed_post in nlp.pipe(get_post(\"data/text.txt\"),batch_size=10, n_threads=3):\n",
    "    nested = []\n",
    "    for sent in parsed_post.sents:\n",
    "        for token in sent:\n",
    "            if not (token.is_punct or token.is_space or token.is_stop or token.like_num or token.like_email or token.like_url):\n",
    "                nested.append(token.lemma_)\n",
    "    nest.append(nested)            \n",
    "print(nested)\n",
    "\n",
    "        \n",
    "##Vecchia funzione\n",
    "#def lemmatize_corpus(filename):\n",
    "#    for parsed_review in nlp.pipe(get_review(filename),batch_size=10, n_threads=3):\n",
    "#        for sent in parsed_review.sents:\n",
    "#            yield u' '.join([token.lemma_ for token in sent if not (token.is_punct or token.is_space or token.is_stop or token.like_num or token.like_email or token.like_url)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's see an excerpt of what we got out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cerro comedor cordobes macri prometido ayuda luis almadaes cordobes ytenia comedor comunitario fundacion ayudar gente situacion calle desesperado mayo envio mensaje redes sociales presidente mauricio macri ayude sostener emprendimiento mandatario llamo telefono julio vieron personalmente provincia prometio ayuda llego alternativa bajar persiana fundacion ayudo amigo alma ayudaba personas situacion calle vendieran golosinas peatonal pudieran capacitarse oficio almada nacionque debio cerrar fundacion puse esfuerzo cumpli comprometi llame presidente pedirle alfajores ayudara tratamiento adicciones asistentes sociales macri hagamos tiempos nacion mismos gente pateando primero meses semana pasada meses -continuo- puse plata bolsillo pedi gente vino presidente laburo tendrian almada requisitos requisitos piden salon sale $ aparte pagando provincia alojamiento muchachos municipalidad permiso venta callejera proyecto quedo rengo conto hablo telefonicamente ministra carolina stanley señalo envio asesora reuni concejal presidente palabra vale almada dijimos empezara contar plata positivo muchachos empece trabajo laburo diez sociedad pelota cancha visita presidencial cordobes conto presidente carancheamos pollos fuente luego regalo camiseta talleres nombre presidente embargo luego anunciar cese actividades fundacion almada emitio comunicado traves redes sociales afirma seguira trabajando estado nacional ayudar personas situacion calle recibimos pedido desesperado gente ayudabamos comunicaron personas ganas involucrarse ayudar obliga seguirlos ayudando reevaluar estrategia explica comunicado ayudo amigo alma ratifica seguir trabajando estado nacional ayudar personas situacion calle cordoba recibimos pedido desesperado gente ayudabamos comunicaron personas ganas involucrarse ayudar obliga seguirlos ayudando reevaluar estrategia hablamos gobierno comprometieron ayudarnos ministerio desarrollo social nacion habian pedido requisitos transparente tuvimos salimos gente pudimos dimos\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('data/parsed_text.txt',encoding='utf-8') as parsed_text:    \n",
    "    print(parsed_text.read(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cerro comedor cordobes macri prometido ayuda luis almadaes cordobes ytenia comedor comunitario fundacion ayudar gente situacion calle \n",
      "desesperado mayo envio mensaje redes sociales presidente mauricio macri ayude sostener emprendimiento \n",
      "mandatario llamo telefono julio vieron personalmente provincia \n",
      "prometio ayuda \n",
      "llego alternativa bajar persiana \n",
      "fundacion ayudo amigo alma ayudaba personas situacion calle vendieran golosinas peatonal pudieran capacitarse oficio \n",
      "almada nacionque debio cerrar fundacion puse esfuerzo cumpli comprometi \n",
      "llame presidente pedirle alfajores ayudara tratamiento adicciones asistentes sociales \n",
      "macri hagamos \n",
      "tiempos nacion mismos gente \n",
      "pateando primero meses semana pasada meses -continuo- \n",
      "puse plata bolsillo \n",
      "pedi gente \n",
      "vino presidente laburo tendrian \n",
      "almada requisitos requisitos piden salon sale $ aparte pagando provincia alojamiento muchachos municipalidad permiso venta callejera \n",
      "proyecto quedo rengo conto \n",
      "hablo telefonicamente ministra carolina sta\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('data/parsed_phrases.txt',encoding='utf-8') as parsed_text:    \n",
    "    print(parsed_text.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling with Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Phrase modeling__ is a form of text manipulation that consists in producing new one-word tokens from two or more token. As we saw in named entity recognition, there are groups of words that represent things that have nothing to do with the single words themselves that make up the group. For example *New York* is supposed to be different in meaning from *New* and *York*. We would like to have these single token words joined together in a single word, with an underscore instead of a space. We then repeat the process (__second-order phrase modeling__) to catch three-word tokens such as *New_York_City*.\n",
    "\n",
    "We will use [__gensim__](https://radimrehurek.com/gensim/index.html), an incredible Python library that implements several unsupervised machine learning algorithms designed for text analysis and also some useful text manipulation classes. To accomplish phrase modeling, gensim provides automatic common phrase detection (multiword expressions) from a stream of sentences. The phrases are identified as __collocations__ (frequently co-occurring tokens). In the built-in [*gensim.models.phrases.Phrases*](https://radimrehurek.com/gensim/models/phrases.html) class there are actually two ways ([formulas](https://radimrehurek.com/gensim/models/phrases.html#id2)) for measuring the co-occurrence of these composite words in the corpus, meaning the __frequency__ these words appear __together__ in sequence, compared to the frequency they appear __alone__. We will use the *default* mode, with a *threshold* set to *170*, just enough to catch *mauricio_macri* as a phrase.\n",
    "\n",
    "Gensim's [gensim.models.word2vec.LineSentence](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence) class provides a convenient iterator for working with other gensim components. It streams the sentences from the disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora.\n",
    "\n",
    "Finally, we write the sentences to a new txt file, *bigram_sents.txt*, and we snip the head to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#Supress useless warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    #Creates iterable of sentences\n",
    "    unigram_sents = LineSentence('data/parsed_phrases.txt')\n",
    "\n",
    "    #Initialize the model with our dataset\n",
    "    bigram_model = Phrases(unigram_sents, threshold=170)\n",
    "\n",
    "    #Save and load trained model in data directory (optional)\n",
    "    bigram_model.save('data/bigram_model')\n",
    "    bigram_model = Phrases.load('data/bigram_model')\n",
    "\n",
    "    #Write processed sentences to the new file \n",
    "    with codecs.open('data/bigram_sents.txt','w',encoding='utf-8') as f:\n",
    "        for sent in unigram_sents:\n",
    "            bigram_sent = u' '.join(bigram_model[sent])\n",
    "            f.write(bigram_sent + '\\n')\n",
    " #   for sent in unigram_sents:\n",
    " #       print(sent)\n",
    "    #Write processed sentences to the new file \n",
    "    with codecs.open('data/bigram_posts.txt','w',encoding='utf-8') as f:\n",
    "        for sent in unigram_sents:\n",
    "            bigram_sent = u' '.join(bigram_model[sent])\n",
    "            f.write(bigram_sent + '\\n')\n",
    "for sent in unigram_sents:\n",
    "    print(sent)\n",
    " #Print the first 3000 characters\n",
    "    #with codecs.open('data/bigram_posts.txt',encoding='utf-8') as f:    \n",
    "    #    print(f.read(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gensim has picked up some composite __phrases__ that we __expected__ such as *mauricio_macri*, *policia_federal*, *codigo_penal* etc. and some __unexpected__ ones such as *acusado_cometer*, *universitario_anibal* etc. With a correct lemmatization this process is far more accurate because inflections in the words account for a larger number of tokens and the co-occurrence model produces more __false positives__. Still, adjusting the threshold to *170* has somewhat mitigated the false processing, producing a functional text.\n",
    "\n",
    "At this point this step is replicated another time, to join three word tokens (second-order phrase modeling). For example if the first pass has joined words like *new* and *york*, producing *new_york*, with a second pass we would expect to join *new_york* and *city*, getting *new_york_city*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 329 ms, sys: 611 µs, total: 330 ms\n",
      "Wall time: 327 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Supress useless warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    #Creates iterable of sentences from our two-word token dataset\n",
    "    bigram_sents = LineSentence('data/bigram_sents.txt')\n",
    "\n",
    "    #Initialize the model with our dataset\n",
    "    trigram_model = Phrases(bigram_sents, threshold=500)\n",
    "\n",
    "    #Save and load trained model in data directory (optional)\n",
    "    trigram_model.save('data/trigram_model')\n",
    "    trigram_model = Phrases.load('data/trigram_model')\n",
    "    \n",
    "    #Write processed sentences to the new file\n",
    "    with codecs.open('data/trigram_sents.txt','w',encoding='utf-8') as f:\n",
    "        for sent in bigram_sents:\n",
    "            trigram_sentence = u' '.join(trigram_model[sent])\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually really useful in our case, because in Argentina people usually have either a middle name or two last names, and in these cases we are left with a one-word token! For example you see appearing in our corpus words such as *maría_eugenia_vidal* and *alejandra_gils_carbó*. Notice that I chose a higher threshold, *400*, to reduce the number of false positives.\n",
    "\n",
    "Our corpus is actually ready for topic modeling, but before proceding let's see how the text changed. Let's print the same post before and after text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Cerró el comedor cordobés al que Macri le había prometido ayuda Luis Almadaes cordobés ytenía un comedor comunitario y una fundación para ayudar a gente en situación de calle. Desesperado, en mayo le envió un mensaje por redes sociales al presidente Mauricio Macri para que lo ayude a sostener el emprendimiento. El mandatario lo llamó por teléfono días después y el 12 de julio se vieron personalmente en esa provincia. Allí, le prometió ayuda. Pero nunca llegó y por eso no tuvo más alternativa que bajar la persiana. Su fundación \"Yo Te Ayudo Amigo del Alma\" ayudaba a unas 20 personas en situación de calle para que vendieran golosinas en la peatonal y pudieran capacitarse en un oficio. Almada dijo a La Naciónque debió cerrar la fundación: \"Puse mi esfuerzo,cumplí con lo que me comprometí pero solo no puedo. No llamé al Presidente para pedirle alfajores, sino para que nos ayudara con tratamiento de adicciones, con asistentes sociales. Macri me dijo que lo hagamos.Los tiempos de Nación no son los mismos que los de la gente\". \"Me fueron pateando primero dos meses, después tres y la semana pasada me dijeron cuatro meses -continuó-. Puse plata de mi bolsillo, pero no puedo más. No pedí nada para mí, era para la gente. Vino el Presidente; estamos haciendo un laburo donde tendrían que estar ellos\".  Almada dijo que son \"requisitos, y más requisitos, y nos piden un salón que sale $20.000 aparte del que estamos pagando; la Provincia nos dio alojamiento para los muchachos y la Municipalidad nos dio el permiso para la venta callejera. El proyecto quedó rengo\", contó. Hace unos días habló telefónicamente con la ministra Carolina Stanley\n",
      "________________________________________________\n",
      "After:\n",
      "cerro comedor cordobes macri prometido ayuda luis almadaes cordobes ytenia comedor comunitario fundacion ayudar gente situacion calle\n",
      "desesperado mayo envio mensaje redes_sociales presidente mauricio_macri ayude sostener emprendimiento\n",
      "mandatario llamo telefono julio vieron personalmente provincia\n",
      "prometio ayuda\n",
      "llego alternativa bajar persiana\n",
      "fundacion ayudo amigo alma ayudaba personas situacion calle vendieran golosinas peatonal pudieran capacitarse oficio\n",
      "almada nacionque debio cerrar fundacion puse esfuerzo cumpli comprometi\n",
      "llame presidente pedirle alfajores ayudara tratamiento adicciones asistentes sociales\n",
      "macri hagamos\n",
      "tiempos nacion mismos gente\n",
      "pateando primero meses semana pasada meses -continuo-\n",
      "puse plata bolsillo\n",
      "pedi gente\n",
      "vino presidente laburo tendrian\n",
      "almada requisitos requisitos piden salon sale $ aparte pagando provincia alojamiento muchachos municipalidad permiso venta callejera\n",
      "proyecto quedo rengo conto\n",
      "hablo telefonicamente ministra carolina stanley señalo envio asesora\n",
      "reuni concejal presidente palabra vale almada dijimos empezara\n",
      "contar\n",
      "plata positivo muchachos empece trabajo\n",
      "laburo diez sociedad pelota cancha\n",
      "visita presidencial cordobes conto presidente carancheamos pollos fuente\n",
      "luego regalo camiseta talleres nombre presidente\n",
      "embargo luego anunciar cese actividades fundacion almada emitio comunicado traves redes_sociales afirma seguira trabajando estado nacional ayudar personas situacion calle\n",
      "recibimos pedido desesperado gente ayudabamos comunicaron personas ganas involucrarse ayudar\n",
      "obliga seguirlos ayudando reevaluar estrategia explica comunicado\n",
      "ayudo amigo alma ratifica seguir trabajando estado nacional ayudar personas situacion calle cordoba\n",
      "recibimos pedido desesperado gente ayudabamos comunicaron personas ganas involucrarse ayudar\n",
      "obliga seguirlos ayudando reevaluar estrategia\n",
      "hablamos gobierno comprometieron ayudarnos\n",
      "ministerio desarrollo social nacion habian pedido requisitos transparente\n",
      "tuvimos salimos gente pudimos dimos\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\")\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as f:\n",
    "    print(f.read(845))\n",
    "print(\"________________________________________________\\nAfter:\")\n",
    "with codecs.open('data/trigram_sents.txt',encoding='utf-8') as f:\n",
    "    print(f.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a dictionary with Gensim\n",
    "Now that our text is ready to go we can use it to build a gensim [*dictionary*](https://radimrehurek.com/gensim/corpora/dictionary.html), which, in gensim's jargon, consists in a mapping between *words* and their integer *ids*. Dictionaries created from a corpus can later be pruned according to document frequency (removing (un)common words), save/loaded from disk (via *Dictionary.save()* and *Dictionary.load()* methods), merged with other dictionary (*Dictionary.merge_with()*) etc.\n",
    "\n",
    "Dictionary keys in gensim, like in python, constitute a set, thus contain one instance of every word. There are some words that we are not interested in for topic modeling, such as too common or too uncommon words. We can remove them from our dictionary via the *Dictionary.filter_extremes()* method. After some tokens have been removed via there are gaps in the id series. Calling this *Dictionary.compactify()* method will remove these gaps and reassign integer ids.\n",
    "\n",
    "Then we call the *doc2bow()* function to parse our posts and yield a bag-of-words set. In this \"casting\" the sequential relationship between words is lost, but the number of occurrences of each word of the post is stored in a [vector](https://radimrehurek.com/gensim/tut1.html). We pass the bow set to the [*MmCorpus.serialize()*](https://radimrehurek.com/gensim/corpora/mmcorpus.html) function that iterates through the document stream corpus and saves the bow representation in a simple [Market Matrix](http://math.nist.gov/MatrixMarket/formats.html) format to the disk. Gensim also supports [other formats](https://radimrehurek.com/gensim/tut1.html) such as [Joachim's *SVMlight* format](http://svmlight.joachims.org/), [Blei's LDA-C](http://www.cs.columbia.edu/~blei/lda-c/) format and [GibbsLDA++](http://gibbslda.sourceforge.net/) format.\n",
    "\n",
    "We then easily load the matrix in a variable calling *MmCorpus()*, we will use this variable for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:\n",
      "Dictionary(14209 unique tokens: ['cerro', 'comedor', 'cordobes', 'macri', 'prometido']...)\n",
      "\n",
      "After filtering\n",
      "Dictionary(14209 unique tokens: ['cerro', 'comedor', 'cordobes', 'macri', 'prometido']...) \n",
      "\n",
      "CPU times: user 470 ms, sys: 3.4 ms, total: 474 ms\n",
      "Wall time: 468 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "#Set up sentence streaming\n",
    "trigram_posts = LineSentence('data/trigram_sents.txt')\n",
    "\n",
    "#Learn the dictionary by iterating over all of the posts\n",
    "trigram_dictionary = Dictionary(trigram_posts)\n",
    "\n",
    "#Get infos about our dict before filtering\n",
    "print(\"Before filtering:\")\n",
    "print(trigram_dictionary)\n",
    "\n",
    "#Filter out words that appear in less than 6 posts or more than 80% posts\n",
    "trigram_dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#Get infos about our dict after filtering\n",
    "print(\"\\nAfter filtering\")\n",
    "print(trigram_dictionary,'\\n')\n",
    "\n",
    "#Print tokens after filtering\n",
    "#print(trigram_dictionary.token2id)\n",
    "\n",
    "#Generate new ids \n",
    "trigram_dictionary.compactify()\n",
    "   \n",
    "#Save and load the finished dictionary from in data directory (optional)\n",
    "trigram_dictionary.save('data/trigram_dict.dict')\n",
    "trigram_dictionary = Dictionary.load('data/trigram_dict.dict')\n",
    "\n",
    "#Read the posts and generate bag-of-words representation\n",
    "corpus = [trigram_dictionary.doc2bow(post) for post in trigram_posts]\n",
    "\n",
    "#Print post vectors\n",
    "#print(corpus)\n",
    "\n",
    "#Save the bow corpus as a matrix\n",
    "MmCorpus.serialize('data/trigram_bow_corpus_all.mm',corpus)\n",
    "\n",
    "#Load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus('data/trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with Gensim\n",
    "What is a topic model? Why would you want to create? Gensim's creator, Radim Rehurek, gives two reasonable answers:\n",
    "- To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.\n",
    "- To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy (marginal data trends are ignored, noise-reduction).\n",
    "\n",
    "As a matter of fact, the main problem with topic modeling (and other NLP task) is that we represent documents (posts) as vector spaces of tokens, and since the __dimension__ of the document vectors is the number of tokens in the corpus vocabulary, it ends up being very __large__. Furthermore every document contains only a small fraction of all tokens in the vocabulary, thus they also tend to be very __sparse__. What we can do is create a new conceptual layer built in our model. Instead of using tokens directly in documents, we describe everything in term of topics: documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. \n",
    "\n",
    "Gensim provides different algorithms for training a topic model. Now that we created a corpus of documents represented as a stream of vectors we can treat it with different transformations. We are going to try all the transformations available in gensim, starting from the simpler ones and building on them, because [transformation](https://radimrehurek.com/gensim/tut2.html) can be stacked. In order we will train our corpus using:\n",
    "- [__Tf-idf__](https://radimrehurek.com/gensim/models/tfidfmodel.html) (Term frequency - inverse document frequency)\n",
    "- [__LSI__](https://radimrehurek.com/gensim/models/lsimodel.html) (Latent Semantic Indexing) aka LSA\n",
    "- [__RP__](https://radimrehurek.com/gensim/models/rpmodel.html) (Random Projections)\n",
    "- [__HDP__](https://radimrehurek.com/gensim/models/hdpmodel.html) (Hierarchical Dirichlet Process)\n",
    "- [__LDA__](https://radimrehurek.com/gensim/models/ldamodel.html) (Latent Dirichlet Allocation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf (Term frequency - inverse document frequency)\n",
    "[__Tf-idf__](https://radimrehurek.com/gensim/models/tfidfmodel.html) is the naivest form of training we can do. The way it works is somewhat similar to what we have already done when we created the bag-of-words, but this time the frequencies computed will be a real number:\n",
    "- __Term frequency__: Counts the number of occurencies (frequency) of each term appearing in the dictionary.\n",
    "- __Inverse document frequency__: Introduces a factor that diminishes the weight of terms that occur very frequently in the corpus and increases the weight of terms that occur rarely.\n",
    "\n",
    "In gensim transformations are standard Python objects, typically initialized by means of a training corpus. In case of tf-idf, the \"training\" consists simply of going through the supplied corpus once and computing document frequencies of all its items, increasing the value of rare tokens. In this particular case, we are transforming the same corpus that we used for training, but this is only incidental. Once the transformation model has been initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used in the training corpus at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 242 ms, sys: 6.61 ms, total: 249 ms\n",
      "Wall time: 358 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "#Count the frequencies of all tokens in the corpus (training)\n",
    "tfidf = TfidfModel(trigram_bow_corpus)\n",
    "\n",
    "#Save and load trained tf-idf corpus\n",
    "tfidf.save('data/trainedcorpus.tfidf_model')\n",
    "tfidf = tfidf.load('data/trainedcorpus.tfidf_model')\n",
    "\n",
    "#Transform our corpus using trained corpus\n",
    "#print(tfidf)\n",
    "corpus_tfidf = tfidf[trigram_bow_corpus]\n",
    "#for doc in corpus_tfidf:\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see *tf-idf* has correctly parsed our corpus, transforming from a bag-of-words __integer__ frequency representation to a tf-idf __real-valued__ frequency weighted matrix, increasing the frequency of rare tokens.\n",
    "\n",
    "We can now use this new *corpus_tfidf* to train other topic mining algorithms instead of the simpler bag-of-words representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI (Latent Semantic Indexing)\n",
    "[__LSI__](https://radimrehurek.com/gensim/models/lsimodel.html) transforms our corpus from Tf-Idf weighted space into a latent space of a lower dimensionality. The \"latency\" is supposed to represent a hidden connection between words (topics, indeed) and can be set at runtime via the *num_topics* parameter. We also turn *onepass* parameter off to force a multi-pass stochastic algorithm and increase *power_iters* and *extra_samples* that represent the number of power iterations  and an oversampling factor respectively, to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14209 unique tokens: ['cerro', 'comedor', 'cordobes', 'macri', 'prometido']...)\n",
      "\n",
      "term                 frequency\n",
      "macri                0.238\n",
      "pampa                0.200\n",
      "presidente           0.159\n",
      "huevazos             0.138\n",
      "mauricio             0.125\n",
      "gils                 0.125\n",
      "carbo                0.123\n",
      "inflacion            0.115\n",
      "huevos               0.115\n",
      "docente              0.115\n",
      "cristina             0.107\n",
      "federal              0.104\n",
      "policia              0.103\n",
      "gobierno             0.102\n",
      "santa                0.099\n",
      "\n",
      "term                 frequency\n",
      "gils                 -0.403\n",
      "carbo                -0.400\n",
      "pampa                0.199\n",
      "huevazos             0.143\n",
      "macri                0.143\n",
      "edificio             -0.141\n",
      "compra               -0.133\n",
      "procuracion          -0.124\n",
      "procuradora          -0.121\n",
      "docente              0.115\n",
      "huevos               0.115\n",
      "procesamiento        -0.111\n",
      "alejandra            -0.110\n",
      "rosa                 0.094\n",
      "licitacion           -0.088\n",
      "\n",
      "term                 frequency\n",
      "gils                 0.266\n",
      "carbo                0.264\n",
      "inflacion            -0.245\n",
      "pampa                0.169\n",
      "cristina             -0.154\n",
      "huevazos             0.126\n",
      "docente              0.125\n",
      "kirchner             -0.110\n",
      "coloquio             -0.109\n",
      "mar                  -0.105\n",
      "federal              0.100\n",
      "edificio             0.097\n",
      "huevos               0.095\n",
      "idea                 -0.095\n",
      "plata                -0.092\n",
      "\n",
      "term                 frequency\n",
      "maldonado            0.415\n",
      "bono                 0.344\n",
      "santiago             0.291\n",
      "carta                0.200\n",
      "inflacion            -0.199\n",
      "familia              0.157\n",
      "u2                   0.142\n",
      "cantante             0.115\n",
      "banda                0.093\n",
      "amnistia             0.093\n",
      "rindan               0.089\n",
      "desaparicion_forzada 0.087\n",
      "carrio               0.087\n",
      "vivo                 0.085\n",
      "sergio               0.084\n",
      "\n",
      "term                 frequency\n",
      "vivo                 -0.423\n",
      "trabajando_tengas    -0.340\n",
      "experiencia_radios_tienes -0.340\n",
      "problemas_reproduccion_audio -0.340\n",
      "favor_refresca_pagina -0.340\n",
      "transmision          -0.211\n",
      "hd                   -0.190\n",
      "mejor                -0.176\n",
      "fm                   -0.162\n",
      "queres               -0.153\n",
      "momento              -0.138\n",
      "musica               -0.134\n",
      "radio                -0.127\n",
      "maldonado            0.112\n",
      "mitre                -0.080\n",
      "\n",
      "term                 frequency\n",
      "inflacion            0.235\n",
      "vivo                 -0.202\n",
      "trabajando_tengas    0.173\n",
      "problemas_reproduccion_audio 0.173\n",
      "favor_refresca_pagina 0.173\n",
      "experiencia_radios_tienes 0.173\n",
      "florencia            -0.139\n",
      "mapuche              0.134\n",
      "pueblo               0.125\n",
      "transmision          -0.120\n",
      "vidal                -0.110\n",
      "radio                -0.102\n",
      "maldonado            0.100\n",
      "brancatelli          -0.099\n",
      "precios              0.097\n",
      "\n",
      "term                 frequency\n",
      "cristina             0.204\n",
      "kirchner             0.196\n",
      "inflacion            -0.167\n",
      "vidal                0.147\n",
      "brancatelli          0.146\n",
      "precios              -0.146\n",
      "vivo                 -0.145\n",
      "experience_please_fill_out -0.122\n",
      "have_been_receiving_large -0.122\n",
      "for_interruption_we  -0.122\n",
      "continue_with_your_youtube -0.122\n",
      "is_starting_sorry    -0.122\n",
      "form_below           -0.122\n",
      "volume_of_requests_from -0.122\n",
      "next                 -0.122\n",
      "\n",
      "term                 frequency\n",
      "is_starting_sorry    -0.298\n",
      "form_below           -0.298\n",
      "experience_please_fill_out -0.298\n",
      "continue_with_your_youtube -0.298\n",
      "your_network_to      -0.298\n",
      "volume_of_requests_from -0.298\n",
      "have_been_receiving_large -0.298\n",
      "for_interruption_we  -0.298\n",
      "next                 -0.298\n",
      "video                -0.161\n",
      "vivo                 0.090\n",
      "cristina             -0.084\n",
      "kirchner             -0.071\n",
      "transmision          0.058\n",
      "trabajando_tengas    -0.056\n",
      "\n",
      "term                 frequency\n",
      "vivo                 0.521\n",
      "transmision          0.322\n",
      "radio                0.171\n",
      "trabajando_tengas    -0.136\n",
      "experiencia_radios_tienes -0.136\n",
      "problemas_reproduccion_audio -0.136\n",
      "favor_refresca_pagina -0.136\n",
      "cristina             0.104\n",
      "pueblo               0.100\n",
      "afirmacion_aseguraron_vergüenza 0.091\n",
      "tucuman_olga_ramon   0.091\n",
      "mapuche              0.087\n",
      "docente              -0.086\n",
      "apuntar              0.085\n",
      "mejor                -0.085\n",
      "\n",
      "term                 frequency\n",
      "florencia            0.359\n",
      "kirchner             0.265\n",
      "vidal                -0.192\n",
      "brancatelli          -0.174\n",
      "peña                 0.144\n",
      "seguridad            0.113\n",
      "corte_suprema        0.113\n",
      "eugenia              -0.109\n",
      "millones             0.101\n",
      "diego                -0.097\n",
      "felipe               0.096\n",
      "intratables          -0.096\n",
      "embargo              0.096\n",
      "maria                -0.095\n",
      "desestimo_recurso_queja 0.093\n",
      "\n",
      "term                 frequency\n",
      "docente              0.214\n",
      "mapuche              -0.208\n",
      "bono                 0.176\n",
      "pueblo               -0.157\n",
      "inflacion            0.149\n",
      "ataque               0.141\n",
      "kirchnerista         0.128\n",
      "detuvieron           0.113\n",
      "florencia            -0.111\n",
      "maldonado            0.107\n",
      "carta                0.107\n",
      "peña                 -0.105\n",
      "comitiva             -0.102\n",
      "integrante           -0.096\n",
      "anibal_prina         0.095\n",
      "\n",
      "term                 frequency\n",
      "brancatelli          -0.211\n",
      "coloquio             0.184\n",
      "ley                  0.171\n",
      "empresarios          0.170\n",
      "idea                 0.168\n",
      "vidal                -0.149\n",
      "haga_respetar        0.144\n",
      "garantiza            0.138\n",
      "reforma              0.122\n",
      "macri                0.118\n",
      "eugenia              -0.116\n",
      "candidatos           -0.110\n",
      "volver               0.110\n",
      "mar                  0.103\n",
      "intratables          -0.103\n",
      "\n",
      "term                 frequency\n",
      "vuelos               0.171\n",
      "seleccion            -0.143\n",
      "gremios              0.140\n",
      "mundial              -0.140\n",
      "inflacion            -0.135\n",
      "peña                 -0.128\n",
      "messi                -0.125\n",
      "jugadores            -0.121\n",
      "incremento           0.119\n",
      "rusia                -0.116\n",
      "transporte           0.108\n",
      "pasajes              0.108\n",
      "felipe               -0.108\n",
      "aeronauticos         0.103\n",
      "semana               0.101\n",
      "\n",
      "term                 frequency\n",
      "vuelos               0.301\n",
      "gremios              0.235\n",
      "aeronauticos         0.171\n",
      "aerolineas           0.162\n",
      "salarial             0.160\n",
      "conflicto            0.129\n",
      "asambleas            0.122\n",
      "aerolineas_argentinas 0.116\n",
      "paro                 0.113\n",
      "medida               0.108\n",
      "ley                  -0.107\n",
      "ezeiza               0.103\n",
      "aires                -0.102\n",
      "aeroparque           0.100\n",
      "haga_respetar        -0.099\n",
      "\n",
      "term                 frequency\n",
      "peña                 0.186\n",
      "brancatelli          0.180\n",
      "seleccion            -0.177\n",
      "mundial              -0.158\n",
      "kirchner             -0.153\n",
      "radio                -0.148\n",
      "rusia                -0.145\n",
      "messi                -0.144\n",
      "jugadores            -0.141\n",
      "chile                -0.119\n",
      "felipe               0.114\n",
      "eugenia              0.110\n",
      "sampaoli             -0.110\n",
      "intratables          0.103\n",
      "vidal                0.097\n",
      "CPU times: user 1min 31s, sys: 31.2 s, total: 2min 3s\n",
      "Wall time: 55.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "#Train our corpus with lsi\n",
    "lsi = LsiModel(corpus_tfidf,id2word=trigram_dictionary,num_topics=15,onepass=False,power_iters=100)#extra_samples=100)\n",
    "\n",
    "#Save and load the finished LDA model from disk\n",
    "lsi.save('data/lsi_model')\n",
    "lsi = LsiModel.load('data/lsi_model')\n",
    "\n",
    "# Accept a user-supplied topic number and print out a formatted list of the top terms\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    print ('\\n' + '{:20} {}'.format('term','frequency'))\n",
    "    for term, frequency in lsi.show_topic(topic_number, topn=15):\n",
    "        print ('{:20} {:.3f}'.format(term,round(frequency, 3)))\n",
    "print(trigram_dictionary)\n",
    "explore_topic(topic_number=0)\n",
    "explore_topic(topic_number=1)\n",
    "explore_topic(topic_number=2)\n",
    "explore_topic(topic_number=3)\n",
    "explore_topic(topic_number=4)\n",
    "explore_topic(topic_number=5)\n",
    "explore_topic(topic_number=6)\n",
    "explore_topic(topic_number=7)\n",
    "explore_topic(topic_number=8)\n",
    "explore_topic(topic_number=9)\n",
    "explore_topic(topic_number=10)\n",
    "explore_topic(topic_number=11)\n",
    "explore_topic(topic_number=12)\n",
    "explore_topic(topic_number=13)\n",
    "explore_topic(topic_number=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5840 unique tokens: ['cerro', 'comedor', 'cordobes', 'macri', 'prometido']...)\n",
      "term                 frequency\n",
      "\n",
      "coloquio             0.004\n",
      "vos                  0.004\n",
      "reforma              0.004\n",
      "idea                 0.003\n",
      "radio                0.003\n",
      "hotel                0.003\n",
      "gobierno             0.003\n",
      "google               0.003\n",
      "urtubey              0.003\n",
      "cuerpo               0.003\n",
      "mariela              0.003\n",
      "aplicaciones         0.002\n",
      "resolver             0.002\n",
      "play                 0.002\n",
      "cavallo              0.002\n",
      "term                 frequency\n",
      "\n",
      "brancatelli          0.005\n",
      "movil                0.004\n",
      "we_are_sorry_but     0.004\n",
      "vidal                0.004\n",
      "intratables          0.004\n",
      "mujeres              0.003\n",
      "eugenia              0.003\n",
      "farmacia             0.003\n",
      "hd                   0.003\n",
      "audio                0.003\n",
      "round                0.003\n",
      "santiago             0.003\n",
      "maria                0.003\n",
      "robar                0.002\n",
      "encapuchado          0.002\n",
      "term                 frequency\n",
      "\n",
      "chile                0.004\n",
      "maldonado            0.003\n",
      "carrio               0.003\n",
      "incremento           0.003\n",
      "cancer               0.003\n",
      "seleccion            0.003\n",
      "pasajes              0.003\n",
      "argentina            0.002\n",
      "sampaoli             0.002\n",
      "empresarios          0.002\n",
      "circulo_rojo         0.002\n",
      "gente                0.002\n",
      "aires                0.002\n",
      "santiago             0.002\n",
      "empresas             0.002\n",
      "term                 frequency\n",
      "\n",
      "to                   0.004\n",
      "carrio               0.004\n",
      "diario               0.004\n",
      "noticias             0.004\n",
      "crecimiento          0.004\n",
      "argentina            0.003\n",
      "fmi                  0.003\n",
      "instante             0.003\n",
      "do                   0.003\n",
      "barcelona            0.003\n",
      "tribuno              0.003\n",
      "or                   0.003\n",
      "mercado              0.003\n",
      "viene                0.003\n",
      "seleccion            0.002\n",
      "term                 frequency\n",
      "\n",
      "avanza               0.004\n",
      "cajeros_automaticos  0.003\n",
      "ypf                  0.003\n",
      "delito               0.003\n",
      "imaginario           0.003\n",
      "docente              0.003\n",
      "defensa              0.003\n",
      "juicio               0.003\n",
      "foto                 0.003\n",
      "web                  0.003\n",
      "sede                 0.002\n",
      "peronismo            0.002\n",
      "ola                  0.002\n",
      "enferma              0.002\n",
      "perra                0.002\n",
      "term                 frequency\n",
      "\n",
      "seguro_convencido_estaremos 0.006\n",
      "problemas_reproduccion_audio 0.006\n",
      "experiencia_radios_tienes 0.006\n",
      "favor_refresca_pagina 0.006\n",
      "trabajando_tengas    0.006\n",
      "momento              0.004\n",
      "mejor                0.004\n",
      "pablo                0.003\n",
      "mundial              0.003\n",
      "mujer                0.003\n",
      "queres               0.003\n",
      "kilometros           0.003\n",
      "fm                   0.003\n",
      "musica               0.003\n",
      "donante              0.003\n",
      "term                 frequency\n",
      "\n",
      "florencia            0.007\n",
      "kirchner             0.005\n",
      "peña                 0.004\n",
      "produjo              0.003\n",
      "felipe               0.003\n",
      "corte                0.003\n",
      "experience_please_fill_out 0.003\n",
      "continue_with_your_youtube 0.003\n",
      "your_network_to      0.003\n",
      "for_interruption_we  0.003\n",
      "form_below           0.003\n",
      "next                 0.003\n",
      "have_been_receiving_large 0.003\n",
      "is_starting_sorry    0.003\n",
      "volume_of_requests_from 0.003\n",
      "term                 frequency\n",
      "\n",
      "gils                 0.012\n",
      "carbo                0.012\n",
      "pagina12             0.004\n",
      "justicia             0.004\n",
      "edificio             0.004\n",
      "procuradora          0.004\n",
      "procuracion          0.004\n",
      "compra               0.004\n",
      "gobierno             0.004\n",
      "alejandra            0.003\n",
      "fiscales             0.003\n",
      "procesamiento        0.003\n",
      "cargo                0.003\n",
      "proceso              0.003\n",
      "licitacion           0.003\n",
      "term                 frequency\n",
      "\n",
      "radio                0.004\n",
      "tren                 0.004\n",
      "lomas_zamora         0.004\n",
      "dalma                0.004\n",
      "instagram            0.004\n",
      "mujer                0.003\n",
      "videos               0.003\n",
      "cristina             0.003\n",
      "an                   0.003\n",
      "it                   0.003\n",
      "casa                 0.003\n",
      "vivo                 0.003\n",
      "is                   0.003\n",
      "estacion             0.003\n",
      "cabeza               0.002\n",
      "term                 frequency\n",
      "\n",
      "perro                0.004\n",
      "sindicalismo         0.003\n",
      "verbitsky            0.003\n",
      "animales             0.003\n",
      "italia               0.002\n",
      "latorre              0.002\n",
      "natacha_jaitt        0.002\n",
      "izquierda            0.002\n",
      "diego                0.002\n",
      "florencio_randazzo   0.002\n",
      "yanina               0.002\n",
      "animal               0.002\n",
      "maria                0.002\n",
      "lista                0.002\n",
      "entrevista           0.002\n",
      "term                 frequency\n",
      "\n",
      "vivo                 0.008\n",
      "transmision          0.008\n",
      "haga_respetar        0.006\n",
      "ley                  0.005\n",
      "garantiza            0.005\n",
      "volver               0.004\n",
      "brujo_manuel         0.004\n",
      "lanata               0.004\n",
      "unico                0.004\n",
      "frases               0.003\n",
      "estreno              0.003\n",
      "+                    0.003\n",
      "viuda                0.003\n",
      "britanica            0.003\n",
      "emilio               0.003\n",
      "term                 frequency\n",
      "\n",
      "macri                0.009\n",
      "pampa                0.009\n",
      "presidente           0.007\n",
      "huevazos             0.007\n",
      "mauricio             0.005\n",
      "huevos               0.005\n",
      "rosa                 0.004\n",
      "santa                0.004\n",
      "policia              0.004\n",
      "docente              0.004\n",
      "auto                 0.004\n",
      "personas             0.004\n",
      "comitiva             0.004\n",
      "acto                 0.004\n",
      "mapuche              0.003\n",
      "term                 frequency\n",
      "\n",
      "cristina             0.003\n",
      "afip                 0.003\n",
      "kirchner             0.002\n",
      "monotributistas      0.002\n",
      "chocolate            0.002\n",
      "recetas              0.002\n",
      "celulares            0.002\n",
      "macri                0.002\n",
      "veia                 0.002\n",
      "matar                0.002\n",
      "busca                0.002\n",
      "critico              0.002\n",
      "peronismo            0.002\n",
      "siente               0.002\n",
      "barcelona            0.002\n",
      "term                 frequency\n",
      "\n",
      "inflacion            0.010\n",
      "precios              0.004\n",
      "septiembre           0.004\n",
      "gobierno             0.003\n",
      "cristina             0.003\n",
      "ciudadana            0.003\n",
      "meta                 0.003\n",
      "puntos               0.003\n",
      "vuelos               0.003\n",
      "unidad               0.003\n",
      "mes                  0.003\n",
      "semana               0.002\n",
      "meses                0.002\n",
      "presidenta           0.002\n",
      "kirchner             0.002\n",
      "term                 frequency\n",
      "\n",
      "bono                 0.010\n",
      "maldonado            0.007\n",
      "carta                0.006\n",
      "$                    0.004\n",
      "u2                   0.004\n",
      "santiago             0.004\n",
      "familia              0.004\n",
      "manifestacion        0.003\n",
      "anuncio              0.003\n",
      "san                  0.003\n",
      "cantante             0.003\n",
      "encuentro            0.003\n",
      "banda                0.003\n",
      "españa               0.002\n",
      "gendarmeria          0.002\n",
      "CPU times: user 8min 35s, sys: 1min 21s, total: 9min 57s\n",
      "Wall time: 8min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "#import cPickle as pickle\n",
    "\n",
    "#Supress useless warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # workers => sets the parallelism, and should be set to your number of physical cores minus one\n",
    "    lda = LdaMulticore(corpus_tfidf,num_topics=15,id2word=trigram_dictionary,workers=3, passes=100)\n",
    "\n",
    "    #Save and load the finished LDA model from disk\n",
    "    lda.save('data/lda_model_all')\n",
    "    lda = LdaMulticore.load('data/lda_model_all')\n",
    "\n",
    "# Accept a user-supplied topic number and print out a formatted list of the top terms\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    print ('{:20} {}'.format('term','frequency') + '\\n')\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=15):\n",
    "        print ('{:20} {:.3f}'.format(term,round(frequency, 3)))\n",
    "print(trigram_dictionary)\n",
    "explore_topic(topic_number=0)\n",
    "explore_topic(topic_number=1)\n",
    "explore_topic(topic_number=2)\n",
    "explore_topic(topic_number=3)\n",
    "explore_topic(topic_number=4)\n",
    "explore_topic(topic_number=5)\n",
    "explore_topic(topic_number=6)\n",
    "explore_topic(topic_number=7)\n",
    "explore_topic(topic_number=8)\n",
    "explore_topic(topic_number=9)\n",
    "explore_topic(topic_number=10)\n",
    "explore_topic(topic_number=11)\n",
    "explore_topic(topic_number=12)\n",
    "explore_topic(topic_number=13)\n",
    "explore_topic(topic_number=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
